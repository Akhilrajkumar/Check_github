{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arithmetics\n",
    "\n",
    "## convolution and convolution transpose.\n",
    "\n",
    "V. Dumoulin and F. Visin, “A guide to convolution arithmetic for deep learning,” arXiv, vol. stat.ML, Mar. 2016.\n",
    "\n",
    "Notes: good guide for computing output size in convolutional neural networks. You can check <http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html> for some real code.\n",
    "\n",
    "> Finally note that it is always possible to emulate a transposed convolution with a direct convolution. The disadvantage is that it usually involves adding many columns and rows of zeros to the input, resulting in a much less efficient implementation.\n",
    "\n",
    "no idea how to emulate deconv with conv (maybe by adding zeros into the input, making it larger?). But since this is not the usual approach to deal with it, ignore it.\n",
    "\n",
    "\n",
    "> One way to understand the logic behind zero padding is to consider the connectivity pattern of the transposed convolution and use it to guide the design of the equivalent convolution.\n",
    "\n",
    "Essentially, a convolution defines a matrix $C$ of size output x input. When doing convolution, we perform efficient matrix multiplication, by focusing on nonzero elements for each output. When doing transpose convolution, we focus on nonzero elements for each input. In the case of relationship 8, it happens to be the case that nonzero elements for any input can be confined in a k by k kernel. Maybe not true in general.\n",
    "\n",
    "Well in general, this is still true. This is due to locality of kernels. As each output only depends on few inputs, each input only depends on few spatially close outputs.\n",
    "\n",
    "The reason we only need to consider 2D case is because we can decompose 3D convolution used in computer vision into 2D cases. Consider convolving a $D_1$ channel blob to a $D_2$ channel blob. Then we can break down the overall matrix multiplication into $D_2 \\times D_1$ blocks. Such block structure is also there we doing transpose deconvolution.\n",
    "\n",
    "> Like for convolution arithmetic, the dissertation about transposed convolution arithmetic is simplified by the fact that transposed convolution properties don’t interact across axes.\n",
    "\n",
    "this is more like talking about whether output shape, strides etc. are affect across axes. But this is different from whether we can decompose computations across channels.\n",
    "\n",
    "\n",
    "From relationship 8 on, when saying \"tranposed convolution described by ...\", what's after \"described by\" is always formulated in terms of the equivalent convolution for the transpose.\n",
    "\n",
    "Section 4.4 The intuition is that, after adding padding, each input is responsible for more output, thus making the equivalent padding smaller.\n",
    "\n",
    "In these realtionships, if we convolve a $o'$-sized input with parameters $s, k, p$, we get the output, of size $i'$, which is the tranposed conv's input.\n",
    "\n",
    "If we convolve a $i'$-sized input, with parameters $k', s', p'$, then we get the $o'$-sized input above.\n",
    "\n",
    "So let's verify this on Relationship 9.\n",
    "\n",
    "convolve on $o'$: we get output size as \n",
    "\n",
    "$o'' = o' + 2*p - k + 1$, and we have $o'' = i'$.\n",
    "\n",
    "convolve on $i'$: we get output size as \n",
    "\n",
    "$i'' = i' + 2p' - k' + 1 = i' + 2k - 2p -1 - k = i' + k - 1 = 2p$, and we have $i''=o'$.\n",
    "\n",
    "Relationship 9 and 11 are kind of two opposites, more (less) you pad in the input, less (more) you pad when getting back.\n",
    "\n",
    "If I remember correctly, in these equivalent convolutions, you need to tranpose the filter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* p.8: The final outputs of this procedure are called output feature maps.2 If there are multiple input feature maps, the kernel will have to be 3-dimensional – or, equivalently each one of the feature maps will be convolved with a distinct kernel – and the resulting feature maps will be summed up elementwise to produce the output feature map. -- Highlighted Mar 6, 2017\n",
    "* p.18: Like for convolution arithmetic, the dissertation about transposed convolution arithmetic is simplified by the fact that transposed convolution properties don’t interact across axes. -- Highlighted Mar 6, 2017\n",
    "* p.19: One way to put it is to note that the kernel defines a convolution, but whether it’s a direct convolution or a transposed convolution is determined by how the forward and backward passes are computed. -- Highlighted Mar 6, 2017\n",
    "* p.19: Finally note that it is always possible to emulate a transposed convolution with a direct convolution. The disadvantage is that it usually involves adding many columns and rows of zeros to the input, resulting in a much less efficient implementation. -- Highlighted Mar 6, 2017\n",
    "* p.19: The transposed convolution operation can be thought of as the gradient of some convolution with respect to its input, which is usually how transposed convolutions are implemented in practice. -- Highlighted Mar 6, 2017\n",
    "* p.20: One way to understand the logic behind zero padding is to consider the connectivity pattern of the transposed convolution and use it to guide the design of the equivalent convolution. -- Highlighted Mar 6, 2017\n",
    "* p.20: Interestingly, this corresponds to a fully padded convolution with unit strides. -- Highlighted Mar 6, 2017\n",
    "* p.23: one might expect that the transpose of a convolution with s > 1 involves an equivalent convolution with s < 1. As will be explained, this is a valid intuition -- Highlighted Mar 6, 2017\n",
    "* p.23: what fractional strides involve: zeros are inserted between input units, which makes the kernel move around at a slower pace than with unit strides.3 -- Highlighted Mar 6, 2017\n",
    "* p.23: The constraint on the size of the input i can be relaxed by introducing another parameter a ∈ {0, . . . , s − 1} that allows to distinguish between the s different cases that all lead to the same i′: -- Highlighted Mar 6, 2017\n",
    "\n",
    "~~~\n",
    "@article{Dumoulin:2016tc,\n",
    "author = {Dumoulin, V and Visin, F},\n",
    "title = {{A guide to convolution arithmetic for deep learning}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {stat.ML},\n",
    "month = mar,\n",
    "annote = {good guide for computing output size in convolutional neural networks. You can check <http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html> for some real code.\n",
    "\n",
    "> Finally note that it is always possible to emulate a transposed convolution with a direct convolution. The disadvantage is that it usually involves adding many columns and rows of zeros to the input, resulting in a much less efficient implementation.\n",
    "\n",
    "no idea how to emulate deconv with conv (maybe by adding zeros into the input, making it larger?). But since this is not the usual approach to deal with it, ignore it.\n",
    "\n",
    "\n",
    "> One way to understand the logic behind zero padding is to consider the connectivity pattern of the transposed convolution and use it to guide the design of the equivalent convolution.\n",
    "\n",
    "Essentially, a convolution defines a matrix $C$ of size output x input. When doing convolution, we perform efficient matrix multiplication, by focusing on nonzero elements for each output. When doing transpose convolution, we focus on nonzero elements for each input. In the case of relationship 8, it happens to be the case that nonzero elements for any input can be confined in a k by k kernel. Maybe not true in general.\n",
    "\n",
    "Well in general, this is still true. This is due to locality of kernels. As each output only depends on few inputs, each input only depends on few spatially close outputs.\n",
    "\n",
    "The reason we only need to consider 2D case is because we can decompose 3D convolution used in computer vision into 2D cases. Consider convolving a $D_1$ channel blob to a $D_2$ channel blob. Then we can break down the overall matrix multiplication into $D_2 \\times D_1$ blocks. Such block structure is also there we doing transpose deconvolution.\n",
    "\n",
    "> Like for convolution arithmetic, the dissertation about transposed convolution arithmetic is simplified by the fact that transposed convolution properties don{\\textquoteright}t interact across axes.\n",
    "\n",
    "this is more like talking about whether output shape, strides etc. are affect across axes. But this is different from whether we can decompose computations across channels.\n",
    "\n",
    "\n",
    "From relationship 8 on, when saying \"tranposed convolution described by ...\", what's after \"described by\" is always formulated in terms of the equivalent convolution for the transpose.\n",
    "\n",
    "Section 4.4 The intuition is that, after adding padding, each input is responsible for more output, thus making the equivalent padding smaller.\n",
    "\n",
    "In these realtionships, if we convolve a $o'$-sized input with parameters $s, k, p$, we get the output, of size $i'$, which is the tranposed conv's input.\n",
    "\n",
    "If we convolve a $i'$-sized input, with parameters $k', s', p'$, then we get the $o'$-sized input above.\n",
    "\n",
    "So let's verify this on Relationship 9.\n",
    "\n",
    "convolve on $o'$: we get output size as \n",
    "\n",
    "$o'' = o' + 2*p - k + 1$, and we have $o'' = i'$.\n",
    "\n",
    "convolve on $i'$: we get output size as \n",
    "\n",
    "$i'' = i' + 2p' - k' + 1 = i' + 2k - 2p -1 - k = i' + k - 1 = 2p$, and we have $i''=o'$.\n",
    "\n",
    "Relationship 9 and 11 are kind of two opposites, more (less) you pad in the input, less (more) you pad when getting back.\n",
    "\n",
    "If I remember correctly, in these equivalent convolutions, you need to tranpose the filter.\n",
    "\n",
    "\n",
    "\n",
    "},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-03-06T23:33:50GMT},\n",
    "date-modified = {2017-03-07T15:31:47GMT},\n",
    "url = {http://arxiv.org/abs/1603.07285},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Dumoulin/arXiv%202016%20Dumoulin.pdf},\n",
    "file = {{arXiv 2016 Dumoulin.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Dumoulin/arXiv 2016 Dumoulin.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/7F3F87A1-007B-47CC-9193-6AD3D8A50879}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# architecture\n",
    "\n",
    "## 1x1 conv filters\n",
    "\n",
    "M. Lin, Q. Chen, and S. Yan, “Network In Network,” arXiv, Dec. 2013.\n",
    "\n",
    "Notes: Essentially, this paper shows the usefulness of 1x1 conv, for adding additional nonlinearity.\n",
    "\n",
    "people have been trying to add more nonliearity rather than only ReLU. maxout and Network in Network are different apporaches. Maxout is multiple linear filter in parallel, and NIN is multple sequentially. Since people tend to regard deep as a good thing, perhaps NIN will be favored.\n",
    "\n",
    "In section 4.7, the visualization can be done on last feature map, because the last layer of conv is a 1000 way (for ImageNet) out layer, replacing the fc layer, and values in that layer is shows confidence of classification. Check the caffe model for NIN in Caffe Model Zoo to be more clear about this.\n",
    "\n",
    "~~~\n",
    "@article{Lin:2013wb,\n",
    "author = {Lin, Min and Chen, Q and Yan, S},\n",
    "title = {{Network In Network}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2013},\n",
    "month = dec,\n",
    "annote = {Essentially, this paper shows the usefulness of 1x1 conv, for adding additional nonlinearity.\n",
    "\n",
    "people have been trying to add more nonliearity rather than only ReLU. maxout and Network in Network are different apporaches. Maxout is multiple linear filter in parallel, and NIN is multple sequentially. Since people tend to regard deep as a good thing, perhaps NIN will be favored.\n",
    "\n",
    "In section 4.7, the visualization can be done on last feature map, because the last layer of conv is a 1000 way (for ImageNet) out layer, replacing the fc layer, and values in that layer is shows confidence of classification. Check the caffe model for NIN in Caffe Model Zoo to be more clear about this.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-28T18:28:42GMT},\n",
    "date-modified = {2017-03-27T01:12:10GMT},\n",
    "url = {http://arxiv.org/abs/1312.4400},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Lin/arXiv%202013%20Lin.pdf},\n",
    "file = {{arXiv 2013 Lin.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Lin/arXiv 2013 Lin.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/EE78B540-02DF-4DA6-9808-B8D7C8DEC3F1}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## maxout\n",
    "\n",
    "I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, “Maxout Networks,” arXiv, vol. stat.ML, Feb. 2013.\n",
    "\n",
    "Notes: Check Figure 1 for definition of Maxout network.\n",
    "\n",
    "It's kind of dual to \"network in network\".\n",
    "\n",
    "They are extensions to vanila ReLU network, by using k times of parameters.\n",
    "\n",
    "NIN adds k-1 additional layers.\n",
    "\n",
    "Maxout takes max over k feature maps, instead of 1 feature map and 0 (which is what ReLU is doing).\n",
    "\n",
    "\n",
    "\n",
    "In Figure 6, they compared several ways of adding more parameters.\n",
    "\n",
    "One is maxout,\n",
    "\n",
    "Another is \"cross-channel pooling\". I believe, this simply mean, after you get relu units, you do pooling across channels.\n",
    "\n",
    "this is exactly same as maxout, except that you do additional relu. as said in secition 8\n",
    "\n",
    "> The only difference between max- out and max pooling over a set of rectified linear units is that maxout does not include a 0 in the max.\n",
    "\n",
    "Around Fig. 6, they claim \"Large rectifier, no channel pooling\" has around k times more parameters. But I think it would be $k^2$, since if you stack such large layers, input and output will be both k times more. anyway.\n",
    "\n",
    "\n",
    "There are some discussions on maxout's connection to Dropout. Main result is Figure 7 and Figure 8, showing that dropout is doing model averaging, and maxout helps to achieve more accurate model averaging better than tanh.\n",
    "\n",
    "Didn't read Chapter 8.\n",
    "\n",
    "~~~\n",
    "@article{Goodfellow:2013tf,\n",
    "author = {Goodfellow, I J and Warde-Farley, D and Mirza, M and Courville, Aaron and Bengio, Yoshua},\n",
    "title = {{Maxout Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2013},\n",
    "volume = {stat.ML},\n",
    "month = feb,\n",
    "annote = {Check Figure 1 for definition of Maxout network.\n",
    "\n",
    "It's kind of dual to \"network in network\".\n",
    "\n",
    "They are extensions to vanila ReLU network, by using k times of parameters.\n",
    "\n",
    "NIN adds k-1 additional layers.\n",
    "\n",
    "Maxout takes max over k feature maps, instead of 1 feature map and 0 (which is what ReLU is doing).\n",
    "\n",
    "\n",
    "\n",
    "In Figure 6, they compared several ways of adding more parameters.\n",
    "\n",
    "One is maxout,\n",
    "\n",
    "Another is \"cross-channel pooling\". I believe, this simply mean, after you get relu units, you do pooling across channels.\n",
    "\n",
    "this is exactly same as maxout, except that you do additional relu. as said in secition 8\n",
    "\n",
    "> The only difference between max- out and max pooling over a set of rectified linear units is that maxout does not include a 0 in the max.\n",
    "\n",
    "Around Fig. 6, they claim \"Large rectifier, no channel pooling\" has around k times more parameters. But I think it would be k^2, since if you stack such large layers, input and output will be both k times more. anyway.\n",
    "\n",
    "\n",
    "There are some discussions on maxout's connection to Dropout. Main result is Figure 7 and Figure 8, showing that dropout is doing model averaging, and maxout helps to achieve more accurate model averaging better than tanh.\n",
    "\n",
    "Didn't read Chapter 8.\n",
    "},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-04-27T04:17:55GMT},\n",
    "date-modified = {2017-05-05T01:47:27GMT},\n",
    "url = {http://arxiv.org/abs/1302.4389},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Goodfellow/arXiv%202013%20Goodfellow.pdf},\n",
    "file = {{arXiv 2013 Goodfellow.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Goodfellow/arXiv 2013 Goodfellow.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/58F1BE51-6F98-42CF-BE48-5EDA689B3FFE}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## 3x3x3 neural fabrics\n",
    "\n",
    "S. Saxena and J. Verbeek, “Convolutional Neural Fabrics,” presented at the Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, 2016, pp. 4053–4061.\n",
    "\n",
    "Notes: they proposeed a generic CNN architecture, using only 3x3x3 convolutions (3 channels, 3 width, 3 height). They demonstrated that in theory, all CNN can be implemented using such basic operators, and they optimize such generic architecture directly.\n",
    "\n",
    "The idea is good, unifying all CNN architectures. But based on reviwers' comments, seems that the performance is mediocre.\n",
    "\n",
    "~~~\n",
    "@inproceedings{Saxena:2016uc,\n",
    "author = {Saxena, Shreyas and Verbeek, Jakob},\n",
    "title = {{Convolutional Neural Fabrics}},\n",
    "booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},\n",
    "year = {2016},\n",
    "editor = {Lee, Daniel D and Sugiyama, Masashi and von Luxburg, Ulrike and Guyon, Isabelle and Garnett, Roman},\n",
    "pages = {4053--4061},\n",
    "annote = {they proposeed a generic CNN architecture, using only 3x3x3 convolutions (3 channels, 3 width, 3 height). They demonstrated that in theory, all CNN can be implemented using such basic operators, and they optimize such generic architecture directly.\n",
    "\n",
    "The idea is good, unifying all CNN architectures. But based on reviwers' comments, seems that the performance is mediocre.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-05-05T21:13:13GMT},\n",
    "date-modified = {2017-05-05T21:16:31GMT},\n",
    "url = {http://papers.nips.cc/paper/6304-convolutional-neural-fabrics},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Saxena/2016%20Saxena.pdf},\n",
    "file = {{2016 Saxena.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Saxena/2016 Saxena.pdf:application/pdf;2016 Saxena.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Saxena/2016 Saxena.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/F2579EC6-E520-4935-BC0C-824326532458}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## DenseNet\n",
    "\n",
    "G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely Connected Convolutional Networks,” arXiv, vol. cs.CV, Aug. 2016.\n",
    "\n",
    "Notes: Figure 1 and Eq.2 says it all.\n",
    "\n",
    "Notice that in practice it's not as ideal as in Figure 1.\n",
    "\n",
    "Check Figure 2. There are some dense blocks, and each block is linearly connected, without dense connection. This is to save parameters.\n",
    "\n",
    "Another interesting thing is Figure 5 (arxiv v3 version).\n",
    "\n",
    "The explanation about Figure 5 is confusing. I think essentially, they show the connectivity for each of three dense blocks.\n",
    "\n",
    "Each block has one input layer, and 12 output layers.\n",
    "\n",
    "input layer indexed 0, others indexed 1-12.\n",
    "\n",
    "If you check axis labels for three subfigures, vertically, you go from 0 to 12, and horizontally 1-13.\n",
    "\n",
    "so diagonal is not self connection, but the usual linear connection.\n",
    "\n",
    "I think the layer 13 in one block is the layer 0 in the other? (after some pooling, etc.).\n",
    "\n",
    "In block 2 and 3, the first row shows that, there's little connection going from input layer to later layers (except for the immediate later layer).\n",
    "\n",
    "In block 3, last column shows that final output larges depends on later layers, not earlier layers. So maybe there can be better connectivity patterns.\n",
    "\n",
    "~~~\n",
    "@article{Huang:2016wa,\n",
    "author = {Huang, G and Liu, Z and Weinberger, K Q and van der Maaten, L},\n",
    "title = {{Densely Connected Convolutional Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = aug,\n",
    "annote = {Figure 1 and Eq.2 says it all.\n",
    "\n",
    "Notice that in practice it's not as ideal as in Figure 1.\n",
    "\n",
    "Check Figure 2. There are some dense blocks, and each block is linearly connected, without dense connection. This is to save parameters.\n",
    "\n",
    "Another interesting thing is Figure 5 (arxiv v3 version).\n",
    "\n",
    "The explanation about Figure 5 is confusing. I think essentially, they show the connectivity for each of three dense blocks.\n",
    "\n",
    "Each block has one input layer, and 12 output layers.\n",
    "\n",
    "input layer indexed 0, others indexed 1-12.\n",
    "\n",
    "If you check axis labels for three subfigures, vertically, you go from 0 to 12, and horizontally 1-13.\n",
    "\n",
    "so diagonal is not self connection, but the usual linear connection.\n",
    "\n",
    "I think the layer 13 in one block is the layer 0 in the other? (after some pooling, etc.).\n",
    "\n",
    "In block 2 and 3, the first row shows that, there's little connection going from input layer to later layers (except for the immediate later layer).\n",
    "\n",
    "In block 3, last column shows that final output larges depends on later layers, not earlier layers. So maybe there can be better connectivity patterns.},\n",
    "keywords = {context, deep learning},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-05-05T21:33:07GMT},\n",
    "date-modified = {2017-05-05T21:44:43GMT},\n",
    "url = {http://arxiv.org/abs/1608.06993},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Huang/arXiv%202016%20Huang.pdf},\n",
    "file = {{arXiv 2016 Huang.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Huang/arXiv 2016 Huang.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/91C92E38-A304-40E9-A8B8-E8FB26492B1B}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## Fractal Net\n",
    "\n",
    "G. Larsson, M. Maire, and G. Shakhnarovich, “FractalNet: Ultra-Deep Neural Networks without Residuals,” arXiv, vol. cs.CV, May 2016.\n",
    "\n",
    "Notes: An alternative way to get many ensemble networks, compared to ResNet.\n",
    "\n",
    "showed that residual isn't necessary. maybe ensemble is more important.\n",
    "\n",
    "~~~\n",
    "@article{Larsson:2016wd,\n",
    "author = {Larsson, G and Maire, M and Shakhnarovich, G},\n",
    "title = {{FractalNet: Ultra-Deep Neural Networks without Residuals}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = may,\n",
    "annote = {An alternative way to get many ensemble networks, compared to ResNet.\n",
    "\n",
    "showed that residual isn't necessary. maybe ensemble is more important.},\n",
    "keywords = {deep learning, ensemble},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-05-06T01:17:19GMT},\n",
    "date-modified = {2017-05-06T01:19:50GMT},\n",
    "url = {http://arxiv.org/abs/1605.07648},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Larsson/arXiv%202016%20Larsson.pdf},\n",
    "file = {{arXiv 2016 Larsson.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Larsson/arXiv 2016 Larsson.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/37FB120D-36D1-402B-B4EE-11A5CCF5A6AA}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## Deeply Fused Net\n",
    "\n",
    "J. Wang, Z. Wei, T. Zhang, and W. Zeng, “Deeply-Fused Nets,” arXiv, vol. cs.CV, May 2016.\n",
    "\n",
    "Notes: a more general formulation of ResNet, etc.\n",
    "\n",
    "Check Fig 1.\n",
    "\n",
    "Some discussions are confusing.\n",
    "\n",
    "In Section 4 - Concatenation, Maximization, and Summation.\n",
    "\n",
    "> or in the original base network, there are many channels with all 0 entries appended in the output of a block so that the total output matches the size with the input of the subsequent inception module\n",
    "\n",
    "I don't understand this. I know that, if the fusion function is not summation but concatenation, then we will need more parameters, as input to next layer(s) of sub netowrks get larger.\n",
    "\n",
    "I think this 0 channel stuff is saying, instead of increasing number of parameters, you can think number of parameters not changed, except that there are so so many zeros before doing this concatenation fusion (so let's say you merge two networks's output, each with 128 channels; you can say that two networks previously both have 256 channels, but 128 of them are zero)... anyway, weird.\n",
    "\n",
    "~~~\n",
    "@article{Wang:2016tu,\n",
    "author = {Wang, Jianyu and Wei, Z and Zhang, T and Zeng, W},\n",
    "title = {{Deeply-Fused Nets}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = may,\n",
    "annote = {a more general formulation of ResNet, etc.\n",
    "\n",
    "Check Fig 1.\n",
    "\n",
    "Some discussions are confusing.\n",
    "\n",
    "In Section 4 - Concatenation, Maximization, and Summation.\n",
    "\n",
    "> or in the original base network, there are many channels with all 0 entries appended in the output of a block so that the total output matches the size with the input of the subsequent inception module\n",
    "\n",
    "I don't understand this. I know that, if the fusion function is not summation but concatenation, then we will need more parameters, as input to next layer(s) of sub netowrks get larger.\n",
    "\n",
    "I think this 0 channel stuff is saying, instead of increasing number of parameters, you can think number of parameters not changed, except that there are so so many zeros before doing this concatenation fusion (so let's say you merge two networks's output, each with 128 channels; you can say that two networks previously both have 256 channels, but 128 of them are zero)... anyway, weird.},\n",
    "keywords = {deep learning, ensemble},\n",
    "read = {Yes},\n",
    "rating = {2},\n",
    "date-added = {2017-05-06T01:31:45GMT},\n",
    "date-modified = {2017-05-06T01:37:46GMT},\n",
    "url = {http://arxiv.org/abs/1605.07716},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Wang/arXiv%202016%20Wang.pdf},\n",
    "file = {{arXiv 2016 Wang.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Wang/arXiv 2016 Wang.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/F70BC434-EDE1-46E7-AA65-3DE90A8B6752}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## more deeply fused net\n",
    "\n",
    "L. Zhao, J. Wang, X. Li, Z. Tu, and W. Zeng, “On the Connection of Deep Fusion to Ensembling,” arXiv, vol. cs.CV, Nov. 2016.\n",
    "\n",
    "Notes: I think it's similar to \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\", except that here they have some pretty solid performance on ImageNet, and they introduced some new well-performing constructs (merge-and-run).\n",
    "\n",
    "\n",
    "the experiments seem to be pretty extensive.\n",
    "\n",
    "\n",
    "In Section 2, they said that their work is different from \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\", for some reasons.\n",
    "\n",
    "I can't buy first reason:\n",
    "\n",
    "> We show the architec- ture resemblance of the deeply-fused net and an ensemble of neural networks by expanding the deeply-fused net to a structure different from [35]\n",
    "\n",
    "I don't see any difference. For a ResNet with K residual blocks, or a Deeply Fused net with K fusions.\n",
    "\n",
    "I think both of them have $2^K$ paths, as mentioned in 3.1 of this paper, and in Figure 1 of that older paper.\n",
    "\n",
    "~~~\n",
    "@article{Zhao:2016tf,\n",
    "author = {Zhao, L and Wang, Jianyu and Li, X and Tu, Zhuowen and Zeng, W},\n",
    "title = {{On the Connection of Deep Fusion to Ensembling}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = nov,\n",
    "annote = {I think it's similar to \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\", except that here they have some pretty solid performance on ImageNet, and they introduced some new well-performing constructs (merge-and-run).\n",
    "\n",
    "\n",
    "the experiments seem to be pretty extensive.\n",
    "\n",
    "\n",
    "In Section 2, they said that their work is different from \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\", for some reasons.\n",
    "\n",
    "I can't buy first reason:\n",
    "\n",
    "> We show the architec- ture resemblance of the deeply-fused net and an ensemble of neural networks by expanding the deeply-fused net to a structure different from [35]\n",
    "\n",
    "I don't see any difference. For a ResNet with K residual blocks, or a Deeply Fused net with K fusions.\n",
    "\n",
    "I think both of them have 2^K paths, as mentioned in 3.1 of this paper, and in Figure 1 of that older paper.},\n",
    "keywords = {deep learning, ensemble},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-05-06T01:41:20GMT},\n",
    "date-modified = {2017-05-06T02:11:10GMT},\n",
    "url = {http://arxiv.org/abs/1611.07718},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Zhao/arXiv%202016%20Zhao-1.pdf},\n",
    "file = {{arXiv 2016 Zhao-1.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Zhao/arXiv 2016 Zhao-1.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/3E532514-3FE0-4D55-AF53-253B939233B3}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimization\n",
    "\n",
    "## per layer loss\n",
    "\n",
    "C. Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-Supervised Nets,” arXiv, vol. stat.ML, Sep. 2014.\n",
    "\n",
    "Notes: One of early attempts to make gradient pass easier and regularize the hidden layers of deep models.\n",
    "\n",
    "pp. 3\n",
    "\n",
    "> One concern with a direct pursuit of feature discriminativeness at all hidden layers is that this might interfere with the overall network performance, since it is ultimately the feature maps at the output layer which are used for the final classification; our experimental results indicate that this is not the case.\n",
    "\n",
    "Maybe this is a problem, and ResNet solves this.\n",
    "\n",
    "Section 2.2 in equations 3-5, the weight decay terms comes from the soft margin version of SVM, not weight deacy; also, the parameter C is omitted, as mentioned by authors.\n",
    "\n",
    "Section 2.3 here they give some theorectical proof that convergence is faster for DSN. the sentences below Eq. 8 must have some errors. But they don't interfere with reading.\n",
    "\n",
    "Also, below Eq. (8), they define $W_{t+1}$ involving projection. But I think this doesn't affect analysis. In practice, I think this projection is trivial (no projection at all).\n",
    "\n",
    "Some big assumptions in this proof.\n",
    "\n",
    "1. Strongly convex around optimal point.\n",
    "2. P(W) and F(W)=P(W)+Q(W) has same, or very similar optimal solutions, or that we can find a common W* such that P(W*) and F(W*) are very close to their optimal values.\n",
    "\n",
    "I think 2 might be too much. Based on Lemma 1 in the paper, they assume that they can find a solution for Q(W) (here, part of W, as it doesn't have as many layers as P) such that is nearly as small as optimal value for P(W), and then we can build upon this W for Q to find a good W for P.\n",
    "\n",
    "Problem is, can we find good Q(W) close to optimal value of P? For example, I don't think you can find small classification error for conv1, no matter how hard you try.\n",
    "\n",
    "~~~\n",
    "@article{Lee:2014tq,\n",
    "author = {Lee, C Y and Xie, S and Gallagher, P and Zhang, Z and Tu, Z},\n",
    "title = {{Deeply-Supervised Nets}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2014},\n",
    "volume = {stat.ML},\n",
    "month = sep,\n",
    "annote = {One of early attempts to make gradient pass easier and regularize the hidden layers of deep models.\n",
    "\n",
    "\n",
    "pp. 3\n",
    "\n",
    "> One concern with a direct pursuit of feature discriminativeness at all hidden layers is that this might interfere with the overall network performance, since it is ultimately the feature maps at the output layer which are used for the final classification; our experimental results indicate that this is not the case.\n",
    "\n",
    "Maybe this is a problem, and ResNet solves this.\n",
    "\n",
    "Section 2.2 in equations 3-5, the weight decay terms comes from the soft margin version of SVM, not weight deacy; also, the parameter C is omitted, as mentioned by authors.\n",
    "\n",
    "Section 2.3 here they give some theorectical proof that convergence is faster for DSN. the sentences below Eq. 8 must have some errors. But they don't interfere with reading.\n",
    "\n",
    "Also, below Eq. (8), they define $W_{t+1}$ involving projection. But I think this doesn't affect analysis. In practice, I think this projection is trivial (no projection at all).\n",
    "\n",
    "Some big assumptions in this proof.\n",
    "\n",
    "1. Strongly convex around optimal point.\n",
    "2. P(W) and F(W)=P(W)+Q(W) has same, or very similar optimal solutions, or that we can find a common W* such that P(W*) and F(W*) are very close to their optimal values.\n",
    "\n",
    "I think 2 might be too much. Based on Lemma 1 in the paper, they assume that they can find a solution for Q(W) (here, part of W, as it doesn't have as many layers as P) such that is nearly as small as optimal value for P(W), and then we can build upon this W for Q to find a good W for P.\n",
    "\n",
    "Problem is, can we find good Q(W) close to optimal value of P? For example, I don't think you can find small classification error for conv1, no matter how hard you try.},\n",
    "keywords = {deep learning, regularization},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-02-28T18:25:41GMT},\n",
    "date-modified = {2017-03-26T23:37:24GMT},\n",
    "url = {http://arxiv.org/abs/1409.5185},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Lee/arXiv%202014%20Lee.pdf},\n",
    "file = {{arXiv 2014 Lee.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Lee/arXiv 2014 Lee.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/68D3D29D-8D0E-438C-8F1A-7486A26198A7}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet related architecture\n",
    "\n",
    "## highway\n",
    "\n",
    "R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway Networks,” arXiv, vol. cs.LG, May 2015.\n",
    "\n",
    "Notes: similar to ResNet. more general, less successful. This shows that, again, adding parameters may not help.\n",
    "\n",
    "a more general idea of skip connection (resnet), but not working as well as that.\n",
    "\n",
    "~~~\n",
    "@article{Srivastava:2015wn,\n",
    "author = {Srivastava, R K and Greff, K and Schmidhuber, J},\n",
    "title = {{Highway Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2015},\n",
    "volume = {cs.LG},\n",
    "month = may,\n",
    "annote = {similar to ResNet. more general, less successful. This shows that, again, adding parameters may not help.\n",
    "\n",
    "a more general idea of skip connection (resnet), but not working as well as that.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-28T18:36:49GMT},\n",
    "date-modified = {2017-03-27T19:32:13GMT},\n",
    "url = {http://arxiv.org/abs/1505.00387},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Srivastava/arXiv%202015%20Srivastava.pdf},\n",
    "file = {{arXiv 2015 Srivastava.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Srivastava/arXiv 2015 Srivastava.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/6292C75B-91F7-4EE1-8639-8D732DC9F943}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## better residual units\n",
    "\n",
    "K. He, X. Zhang, S. Ren, and J. Sun, “Identity Mappings in Deep Residual Networks,” presented at the Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, 2016, vol. 9908, pp. 630–645.\n",
    "\n",
    "Notes: here different ways of combining two layers in a Residual Unit are explored, and identity mapping seems to help a lot.\n",
    "\n",
    "main rationale is that, while using those tricks such as BN, we should keep the original signal unchanged across layers. This always improve performance.\n",
    "\n",
    "Section 3 shows the advantage of identity mapping, using arguments similar to those in reasoning about gradient explosion and vanishing.\n",
    "\n",
    "Section 3.1 shows decisively that simplest thing is the best, in terms of weights to combine two layers.\n",
    "\n",
    "Section 3.2 explores order of addition, ReLU, BN, so that the non-residual branch is completely unaffected, resulting in pre-activation residual units, as long as the learning capacity of the residual branch is not restricted too much.\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.638: However, this leads to a nonnegative output from the transform F, while intuitively a “residual” function should take values in $(-\\infty, +\\infty)$\n",
    "\n",
    "~~~\n",
    "@inproceedings{He:2016hu,\n",
    "author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n",
    "title = {{Identity Mappings in Deep Residual Networks}},\n",
    "booktitle = {Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV},\n",
    "year = {2016},\n",
    "editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},\n",
    "pages = {630--645},\n",
    "publisher = {Springer},\n",
    "annote = {here different ways of combining two layers in a Residual Unit are explored, and identity mapping seems to help a lot.\n",
    "\n",
    "main rationale is that, while using those tricks such as BN, we should keep the original signal unchanged across layers. This always improve performance.\n",
    "\n",
    "Section 3 shows the advantage of identity mapping, using arguments similar to those in reasoning about gradient explosion and vanishing.\n",
    "\n",
    "Section 3.1 shows decisively that simplest thing is the best, in terms of weights to combine two layers.\n",
    "\n",
    "Section 3.2 explores order of addition, ReLU, BN, so that the non-residual branch is completely unaffected, resulting in pre-activation residual units, as long as the learning capacity of the residual branch is not restricted too much.},\n",
    "keywords = {deep learning},\n",
    "doi = {10.1007/978-3-319-46493-0_38},\n",
    "language = {English},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-03-27T15:03:51GMT},\n",
    "date-modified = {2017-03-27T19:26:19GMT},\n",
    "abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behin},\n",
    "url = {http://dx.doi.org/10.1007/978-3-319-46493-0_38},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/He/2016%20He.pdf},\n",
    "file = {{2016 He.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/He/2016 He.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1007/978-3-319-46493-0_38}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## wider residual nets.\n",
    "\n",
    "S. Zagoruyko and N. Komodakis, “Wide Residual Networks,” arXiv, vol. cs.CV, May 2016.\n",
    "\n",
    "Notes: for a res net, given number of parameters, making each layer wider can have better/worse results than having more layers.\n",
    "\n",
    "On CIFAR, seems that wider is more beneficial. But I would say in the original ResNet paper, the number of channels is simply too few.\n",
    "\n",
    "On ImageNet, it seems that given same number of parameters, it's more useful to have more layers, than having more channels.\n",
    "\n",
    "In pp. 2, \"Width vs depth in residual networks\", it's pointed out that wider networks can be easier to optimize, and people address the problem in Deep res nets by methods such as randomly disabling layers to encourge each residual layer to learn useful information, similar to Dropout\n",
    "\n",
    "Conceptually, this problem is that, given a black box of many hidden layers, how to distribute feature representation of different complexities across layers. This is the similar problem we face when having fc layer, wanting all the units to learn useful representations independently. Therefore, randomly disabling layers is conceptually the same as randomly disabling units for FC layer.\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.2: As gradient flows through the network there is nothing to force it to go through residual block weights and it can avoid learning anything during training, so it is possible that there is either only a few blocks that learn useful representations, or many blocks share very little information with small contribution to the final goal. This problem was formulated as diminishing feature reuse in [28]. The authors of [14] tried to address this problem with the idea of randomly disabling residual blocks during training. This method can be viewed as a special case of dropout [27], where each residual block has an identity scalar weight on which dropout is applied. The effectiveness of this approach proves the hypothesis above.\n",
    "\n",
    "~~~\n",
    "@article{Zagoruyko:2016wo,\n",
    "author = {Zagoruyko, S and Komodakis, N},\n",
    "title = {{Wide Residual Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = may,\n",
    "annote = {for a res net, given number of parameters, making each layer wider can have better/worse results than having more layers.\n",
    "\n",
    "On CIFAR, seems that wider is more beneficial. But I would say in the original ResNet paper, the number of channels is simply too few.\n",
    "\n",
    "On ImageNet, it seems that given same number of parameters, it's more useful to have more layers, than having more channels.\n",
    "\n",
    "In pp. 2, \"Width vs depth in residual networks\", it's pointed out that wider networks can be easier to optimize, and people address the problem in Deep res nets by methods such as randomly disabling layers to encourge each residual layer to learn useful information, similar to Dropout\n",
    "\n",
    "Conceptually, this problem is that, given a black box of many hidden layers, how to distribute feature representation of different complexities across layers. This is the similar problem we face when having fc layer, wanting all the units to learn useful representations independently. Therefore, randomly disabling layers is conceptually the same as randomly disabling units for FC layer.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-28T18:39:15GMT},\n",
    "date-modified = {2017-03-27T19:41:18GMT},\n",
    "url = {http://arxiv.org/abs/1605.07146},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Zagoruyko/arXiv%202016%20Zagoruyko.pdf},\n",
    "file = {{arXiv 2016 Zagoruyko.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Zagoruyko/arXiv 2016 Zagoruyko.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/3423B357-4EBF-45B0-84E4-47E2FA8067F8}},\n",
    "additional-categories = {computer_vision/image_classification}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## residual nets are ensembles\n",
    "\n",
    "A. Veit, M. Wilber, and S. Belongie, “Residual Networks Behave Like Ensembles of Relatively Shallow Networks,” arXiv, vol. cs.CV, May 2016.\n",
    "\n",
    "Notes: Very elegant paper giving insights on the success of ResNet. Essentially, ResNet is powerful because it's ensemble of many relatively independent networks.\n",
    "\n",
    "Also, being deep is not necessary, as effectively, short paths are most critical.\n",
    "\n",
    "However, there's some doubt on Section 4.1. I think VGG doesn't have Batch Normalization layers, making the statistics of different layers very different, and it's doomed to fail. So maybe the comparison is not fair.\n",
    "\n",
    "~~~\n",
    "@article{Veit:2016tc,\n",
    "author = {Veit, A and Wilber, M and Belongie, S},\n",
    "title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = may,\n",
    "annote = {Very elegant paper giving insights on the success of ResNet. Essentially, ResNet is powerful because it's ensemble of many relatively independent networks.\n",
    "\n",
    "Also, being deep is not necessary, as effectively, short paths are most critical.\n",
    "\n",
    "However, there's some doubt on Section 4.1. I think VGG doesn't have Batch Normalization layers, making the statistics of different layers very different, and it's doomed to fail. So maybe the comparison is not fair.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-03-28T21:38:05GMT},\n",
    "date-modified = {2017-03-30T14:58:45GMT},\n",
    "url = {http://arxiv.org/abs/1605.06431},\n",
    "uri = {\\url{papers3://publication/uuid/559EAEE5-EEC0-4CBD-9788-DA7DAF667964}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## multi path residual unit\n",
    "\n",
    "M. Abdi and S. Nahavandi, “Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks,” arXiv, vol. cs.CV, Sep. 2016.\n",
    "\n",
    "Notes: Check Fig 2.\n",
    "\n",
    "Essentially, a variant of ResNet targeting at having more branches, inspired by \"[1]\tA. Veit, M. Wilber, and S. Belongie, “Residual Networks Behave Like Ensembles of Relatively Shallow Networks,” arXiv, vol. cs.CV, May 2016.\".\n",
    "\n",
    "~~~\n",
    "@article{Abdi:2016ua,\n",
    "author = {Abdi, M and Nahavandi, S},\n",
    "title = {{Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = sep,\n",
    "annote = {Check Fig 2.\n",
    "\n",
    "Essentially, a variant of ResNet targeting at having more branches, inspired by \"[1]\tA. Veit, M. Wilber, and S. Belongie, {\\textquotedblleft}Residual Networks Behave Like Ensembles of Relatively Shallow Networks,{\\textquotedblright} arXiv, vol. cs.CV, May 2016.\".},\n",
    "keywords = {deep learning, ensemble},\n",
    "read = {Yes},\n",
    "rating = {2},\n",
    "date-added = {2017-05-06T01:09:42GMT},\n",
    "date-modified = {2017-05-06T01:16:00GMT},\n",
    "url = {http://arxiv.org/abs/1609.05672},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Abdi/arXiv%202016%20Abdi.pdf},\n",
    "file = {{arXiv 2016 Abdi.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Abdi/arXiv 2016 Abdi.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/786BDCAE-2CA5-45DF-9C34-B0F709F79C1F}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inception related architecture\n",
    "\n",
    "## maxout + inception + multiscale\n",
    "\n",
    "Z. Liao and G. Carneiro, “Competitive Multi-scale Convolution,” arXiv, vol. cs.CV, Nov. 2015.\n",
    "\n",
    "Notes: Figure 1 says it all. A combination of maxout and inception.\n",
    "\n",
    "notice that for this structure to work, it seems that batch normalization is needed. See Eq. 2. \n",
    "\n",
    "Otherwise, it's possible that different scales would give different raw magnitudes of filter reponses.\n",
    "\n",
    "~~~\n",
    "@article{Liao:2015te,\n",
    "author = {Liao, Zhibin and Carneiro, Gustavo},\n",
    "title = {{Competitive Multi-scale Convolution}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2015},\n",
    "volume = {cs.CV},\n",
    "month = nov,\n",
    "annote = {Figure 1 says it all. A combination of maxout and inception.\n",
    "\n",
    "notice that for this structure to work, it seems that batch normalization is needed. See Eq. 2. \n",
    "\n",
    "Otherwise, it's possible that different scales would give different raw magnitudes of filter reponses.},\n",
    "keywords = {deep learning, ensemble},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-05-05T16:18:55GMT},\n",
    "date-modified = {2017-05-05T16:25:21GMT},\n",
    "url = {http://arxiv.org/abs/1511.05635},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Liao/arXiv%202015%20Liao.pdf},\n",
    "file = {{arXiv 2015 Liao.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Liao/arXiv 2015 Liao.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/386E1A1F-83BB-4A1C-BD76-C6B7874959E8}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## speedup\n",
    "\n",
    "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the Inception Architecture for Computer Vision,” presented at the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2818–2826.\n",
    "\n",
    "Notes: a lot of speedup tricks. Check figures.\n",
    "\n",
    "writing is horrible (the arxiv version).\n",
    "\n",
    "Some comments on general insights from Section 2.\n",
    "\n",
    "1. \"Theoretically, information content can not be assessed merely by the dimensionality of the representation as it discards important factors like correlation structure\" So dimension is just an auxilary measure on amount of info.\n",
    "2. in general, higher filter output is easier to be trained with smaller kernel size. In my own experience, with 1x1 kernel, and high number of outputs, the training can indeed converge a lot faster (those \"res_proj\" networks in <https://github.com/leelabcnbc/neural_inspired_deep_learning>)\n",
    "3. before performing bigger convolution (3x3 or 5x5), you can first do some 1x1 compression to reduce number of channels. This doesn't seem to affect performance. Probably this is because there's strong spatial correlation among nearby pixels (thus spatial aggregation is really not that useful, compared to channel-wise aggregation)\n",
    "\n",
    "\n",
    "Section 9 shows that with same computation budget, low resoultion can be classified well as well.\n",
    "\n",
    "\n",
    "p.2819: Theoretically, information content can not be assessed merely by the dimensionality of the representation as it discards important factors like correlation structure -- Highlighted May 5, 2017\n",
    "\n",
    "~~~\n",
    "@inproceedings{Szegedy:2016cv,\n",
    "author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},\n",
    "title = {{Rethinking the Inception Architecture for Computer Vision}},\n",
    "booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "year = {2016},\n",
    "pages = {2818--2826},\n",
    "publisher = {IEEE},\n",
    "annote = {a lot of speedup tricks. Check figures. I think many of these trick are covered by CS231n (winter 2016).\n",
    "\n",
    "writing is horrible (the arxiv version).\n",
    "\n",
    "Some comments on general insights from Section 2.\n",
    "\n",
    "1. \"Theoretically, information content can not be assessed merely by the dimensionality of the representation as it discards important factors like correlation structure\" So dimension is just an auxilary measure on amount of info.\n",
    "2. in general, higher filter output is easier to be trained with smaller kernel size. In my own experience, with 1x1 kernel, and high number of outputs, the training can indeed converge a lot faster (those \"res_proj\" networks in <https://github.com/leelabcnbc/neural_inspired_deep_learning>)\n",
    "3. before performing bigger convolution (3x3 or 5x5), you can first do some 1x1 compression to reduce number of channels. This doesn't seem to affect performance. Probably this is because there's strong spatial correlation among nearby pixels (thus spatial aggregation is really not that useful, compared to channel-wise aggregation)\n",
    "\n",
    "\n",
    "Section 9 shows that with same computation budget, low resoultion can be classified well as well.\n",
    "},\n",
    "keywords = {deep learning},\n",
    "doi = {10.1109/CVPR.2016.308},\n",
    "isbn = {978-1-4673-8851-1},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-05-05T16:33:54GMT},\n",
    "date-modified = {2017-05-05T16:48:48GMT},\n",
    "url = {http://ieeexplore.ieee.org/document/7780677/},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Szegedy/CVPR%202016%202016%20Szegedy.pdf},\n",
    "file = {{CVPR 2016 2016 Szegedy.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Szegedy/CVPR 2016 2016 Szegedy.pdf:application/pdf;CVPR 2016 2016 Szegedy.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Szegedy/CVPR 2016 2016 Szegedy.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1109/CVPR.2016.308}}\n",
    "}\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "## extreme speedup\n",
    "\n",
    "F. Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,” arXiv, vol. cs.CV, Oct. 2016.\n",
    "\n",
    "Notes: essentially, they pushed speed up tricks in InceptionV2 paper to the extremes.\n",
    "\n",
    "some details.\n",
    "\n",
    "1. In 4.7, they found that not always having nonlearity is good.\n",
    "2, In 4.1, they talk about JFT, an extremely large dataset.\n",
    "\n",
    "~~~\n",
    "@article{Chollet:2016vb,\n",
    "author = {Chollet, Fran{\\c c}ois},\n",
    "title = {{Xception: Deep Learning with Depthwise Separable Convolutions}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = oct,\n",
    "annote = {essentially, they pushed speed up tricks in InceptionV2 paper to the extremes.\n",
    "\n",
    "some details.\n",
    "\n",
    "1. In 4.7, they found that not always having nonlearity is good.\n",
    "2, In 4.1, they talk about JFT, an extremely large dataset.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {2},\n",
    "date-added = {2017-05-05T16:54:01GMT},\n",
    "date-modified = {2017-05-05T16:56:58GMT},\n",
    "url = {http://arxiv.org/abs/1610.02357},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Chollet/arXiv%202016%20Chollet.pdf},\n",
    "file = {{arXiv 2016 Chollet.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Chollet/arXiv 2016 Chollet.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/0914CE42-4285-4186-A3F9-55DCEA3B8619}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# invarance-inducing architectures\n",
    "\n",
    "## naive scale invarance.\n",
    "\n",
    "A. Kanazawa, A. Sharma, and D. Jacobs, “Locally Scale-Invariant Convolutional Neural Networks,” arXiv, vol. cs.CV, Dec. 2014.\n",
    "\n",
    "Notes: a very naive approach to mutliscale.\n",
    "\n",
    "See Figure 1 and 2.\n",
    "\n",
    "I just suspect the usefulness of this construct on standard datasets... They don't even have experiments on MNIST....\n",
    "\n",
    "~~~\n",
    "@article{Kanazawa:2014ub,\n",
    "author = {Kanazawa, Angjoo and Sharma, Abhishek and Jacobs, David},\n",
    "title = {{Locally Scale-Invariant Convolutional Neural Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2014},\n",
    "volume = {cs.CV},\n",
    "month = dec,\n",
    "annote = {a very naive approach to mutliscale.\n",
    "\n",
    "See Figure 1 and 2.\n",
    "\n",
    "I just suspect the usefulness of this construct on standard datasets... They don't even have experiments on MNIST....},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {2},\n",
    "date-added = {2017-05-05T17:11:21GMT},\n",
    "date-modified = {2017-05-05T17:14:55GMT},\n",
    "url = {http://arxiv.org/abs/1412.5104},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Kanazawa/arXiv%202014%20Kanazawa.pdf},\n",
    "file = {{arXiv 2014 Kanazawa.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Kanazawa/arXiv 2014 Kanazawa.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/2C26B012-F088-413D-86F9-9EC429DF45FF}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## more principled naive scale invariance.\n",
    "\n",
    "Y. Xu, T. Xiao, J. Zhang, K. Yang, and Z. Zhang, “Scale-Invariant Convolutional Neural Networks,” arXiv, vol. cs.CV, Nov. 2014.\n",
    "\n",
    "Notes: similar to \"A. Kanazawa, A. Sharma, and D. Jacobs, “Locally Scale-Invariant Convolutional Neural Networks,” arXiv, vol. cs.CV, Dec. 2014\"\n",
    "\n",
    "except that it's doing scali\u0010ng on filters, not images.\n",
    "\n",
    "also, in the end, concat is used, instead of max.\n",
    "\n",
    "\n",
    "the derivation of this work is principled, see Eq. 2. Basically, an analogy between Eq. 2 and the equation for translation invariance can be made.\n",
    "\n",
    "\n",
    "main problem of such work, is that they really don't work that well on standard datasets. See Section 3.4.\n",
    "\n",
    "the error goes from 11.68-11.35, by using such a technique. it's really not that impressive.\n",
    "\n",
    "~~~\n",
    "@article{Xu:2014wr,\n",
    "author = {Xu, Yichong and Xiao, Tianjun and Zhang, Jiaxing and Yang, Kuiyuan and Zhang, Zheng},\n",
    "title = {{Scale-Invariant Convolutional Neural Networks}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2014},\n",
    "volume = {cs.CV},\n",
    "month = nov,\n",
    "annote = {similar to \"A. Kanazawa, A. Sharma, and D. Jacobs, {\\textquotedblleft}Locally Scale-Invariant Convolutional Neural Networks,{\\textquotedblright} arXiv, vol. cs.CV, Dec. 2014\"\n",
    "\n",
    "except that it's doing scali\u0010ng on filters, not images.\n",
    "\n",
    "also, in the end, concat is used, instead of max.\n",
    "\n",
    "\n",
    "the derivation of this work is principled, see Eq. 2. Basically, an analogy between Eq. 2 and the equation for translation invariance can be made.\n",
    "\n",
    "\n",
    "main problem of such work, is that they really don't work that well on standard datasets. See Section 3.4.\n",
    "\n",
    "the error goes from 11.68-11.35, by using such a technique. it's really not that impressive.},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-05-05T17:35:44GMT},\n",
    "date-modified = {2017-05-05T21:13:51GMT},\n",
    "url = {http://arxiv.org/abs/1411.6369},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Xu/arXiv%202014%20Xu.pdf},\n",
    "file = {{arXiv 2014 Xu.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Xu/arXiv 2014 Xu.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/F597FE1C-2C8F-40EF-AFDE-A58CD59F9375}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## invarance RBM\n",
    "\n",
    "K. Sohn and H. Lee, “Learning Invariant Representations with Local Transformations,” presented at the Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.\n",
    "\n",
    "Notes: dealing with local invarance using sotchastic max pooling (which is invented by Honglak Lee).\n",
    "\n",
    "See Eq. 5, 6.\n",
    "\n",
    "Essentially, take (stochastic) max across a bunch of activations.\n",
    "\n",
    "There is a similar paper \"Kivinen, J. and Williams, C. Transformation equivariant boltzmann machines\".\n",
    "\n",
    "As discussed by the author here, that paper's goal is to have same response under different transformations. But this one, is more like using this local invariance construct to improve classification performance.\n",
    "\n",
    "Due to this goal difference, in that older paper, some parameters in the network are hardcoded, such as rotation matrix, etc. However, here, in princple (and they showed this in experiment), you can learn everything.\n",
    "\n",
    "Essentially, this is a much more solid work than the 2011 ICANN paper.\n",
    "\n",
    "~~~\n",
    "@inproceedings{Sohn:2012ur,\n",
    "author = {Sohn, Kihyuk and Lee, Honglak},\n",
    "title = {{Learning Invariant Representations with Local Transformations}},\n",
    "booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},\n",
    "year = {2012},\n",
    "publisher = {icml.cc / Omnipress},\n",
    "annote = {dealing with local invarance using sotchastic max pooling (which is invented by Honglak Lee).\n",
    "\n",
    "See Eq. 5, 6.\n",
    "\n",
    "Essentially, take (stochastic) max across a bunch of activations.\n",
    "\n",
    "There is a similar paper \"Kivinen, J. and Williams, C. Transformation equivariant boltzmann machines\".\n",
    "\n",
    "As discussed by the author here, that paper's goal is to have same response under different transformations. But this one, is more like using this local invariance construct to improve classification performance.\n",
    "\n",
    "Due to this goal difference, in that older paper, some parameters in the network are hardcoded, such as rotation matrix, etc. However, here, in princple (and they showed this in experiment), you can learn everything.\n",
    "\n",
    "Essentially, this is a much more solid work than the 2011 ICANN paper.\n",
    "\n",
    "},\n",
    "keywords = {deep learning, sparse coding},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-05-05T18:02:21GMT},\n",
    "date-modified = {2017-05-05T18:28:28GMT},\n",
    "url = {http://icml.cc/2012/papers/659.pdf},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2012/Sohn/ICML%202012%202012%20Sohn.pdf},\n",
    "file = {{ICML 2012 2012 Sohn.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2012/Sohn/ICML 2012 2012 Sohn.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/0BB22C9C-0325-4668-A192-16FF787763CA}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis on invariance\n",
    "\n",
    "## one invariance measure\n",
    "\n",
    "I. J. Goodfellow, Q. V. Le, A. M. Saxe, H. Lee, and A. Y. Ng, “Measuring Invariances in Deep Networks,” presented at the Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada., 2009, pp. 646–654.\n",
    "\n",
    "Notes: they proposed a measure to measure invariance. See Section 4.\n",
    "\n",
    "Notice that if the amount of information seen by neurons are different, then their invariance measures can't be compared directly.\n",
    "\n",
    "This is understandable. When you change, say, a 30x30 image, the amount of variation you see in a 5x5 window, is different from that seen from a 10x10 window.\n",
    "\n",
    "Along same arugment, you can also say, that for CNN layers, their invariance measures are not comparable, as they have different RF sizes..\n",
    "\n",
    "\n",
    "So maybe an easier way is to construct some SVM or linear classifer on top of feature map, and use classification rate as invariance measure.\n",
    "\n",
    "\n",
    "Notice that in the test, there's no explicit scale invariance test. But I think you can consider frequency of grating same as scale...\n",
    "\n",
    "~~~\n",
    "@inproceedings{Goodfellow:2009vd,\n",
    "author = {Goodfellow, Ian J and Le, Quoc V and Saxe, Andrew M and Lee, Honglak and Ng, Andrew Y},\n",
    "title = {{Measuring Invariances in Deep Networks}},\n",
    "booktitle = {Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.},\n",
    "year = {2009},\n",
    "editor = {Bengio, Yoshua and Schuurmans, Dale and Lafferty, John D and Williams, Christopher K I and Culotta, Aron},\n",
    "pages = {646--654},\n",
    "publisher = {Curran Associates, Inc.},\n",
    "annote = {they proposed a measure to measure invariance. See Section 4.\n",
    "\n",
    "Notice that if the amount of information seen by neurons are different, then their invariance measures can't be compared directly.\n",
    "\n",
    "This is understandable. When you change, say, a 30x30 image, the amount of variation you see in a 5x5 window, is different from that seen from a 10x10 window.\n",
    "\n",
    "Along same arugment, you can also say, that for CNN layers, their invariance measures are not comparable, as they have different RF sizes..\n",
    "\n",
    "\n",
    "So maybe an easier way is to construct some SVM or linear classifer on top of feature map, and use classification rate as invariance measure.\n",
    "\n",
    "\n",
    "Notice that in the test, there's no explicit scale invariance test. But I think you can consider frequency of grating same as scale...\n",
    "},\n",
    "keywords = {benchmark, deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-05-05T19:15:31GMT},\n",
    "date-modified = {2017-05-05T19:39:53GMT},\n",
    "url = {http://papers.nips.cc/paper/3790-measuring-invariances-in-deep-networks},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2009/Goodfellow/NIPS%202009%202009%20Goodfellow.pdf},\n",
    "file = {{NIPS 2009 2009 Goodfellow.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2009/Goodfellow/NIPS 2009 2009 Goodfellow.pdf:application/pdf;NIPS 2009 2009 Goodfellow.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2009/Goodfellow/NIPS 2009 2009 Goodfellow.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/56AF576C-16AC-4A14-B6F1-1F9A259F689C}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## using clustering (more general, reconstruction) for regularization\n",
    "\n",
    "R. Liao, A. G. Schwing, R. S. Zemel, and R. Urtasun, “Learning Deep Parsimonious Representations,” presented at the Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, 2016, pp. 5076–5084.\n",
    "\n",
    "Notes:\n",
    "\n",
    "The idea is really simple. Regularizing hidden layers so that features in those layers form clusters.\n",
    "\n",
    "Basic problem is that, the experiments are too simple and not convincing. Not sure if it would work on more realistic cases, such VGG + ImageNet.\n",
    "\n",
    "Also, I wonder if some baseline experiments are missing. For example, one very similar approach would be using an autoencoder architecture, where we add a classifcation layer from the middle hidden code layer, and train reconstruction and classification together. Such ideas are discussed in Section 15.1.1 of the Deep Learning book.\n",
    "\n",
    "> It is also possible to train an autoencoder or generative model at the same time as the supervised model. Examples of this single-stage approach include the discriminative RBM (Larochelle and Bengio, 2008) and the ladder network (Rasmus et al., 2015), in which the total objective is an explicit sum of the two terms (one using the labels and one only using the input).\n",
    "\n",
    "I'm not sure the performance of these approaches. Maybe people have long abandoned unsupervised approaches and thus Raquel and her people don't have some state-of-the-art systems using this single-stage approach with autoencoder or other generative models to compare.\n",
    "\n",
    "Some errors. at end of pp. 2, Section 3.1, the i,j mode matrix unfolding doesn't give you many concatenations of i and j mode vectors. instead, it gives a 2d slice, enumerating all indices on i and j dimensions.\n",
    "\n",
    "Some details.\n",
    "\n",
    "at end of Section 3, they say that some cluster centers are drifting, and they need to fix them every some iterations. I think many procedures here are hand wavy, and may need finetuning.\n",
    "\n",
    "Section 4.4 zero shot learning. I believbe Eq. (5) is just some standard formulation in zero-shot learning, and we don't need to care about it.\n",
    "\n",
    "~~~\n",
    "@inproceedings{Liao:2016wi,\n",
    "author = {Liao, Renjie and Schwing, Alexander G and Zemel, Richard S and Urtasun, Raquel},\n",
    "title = {{Learning Deep Parsimonious Representations}},\n",
    "booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},\n",
    "year = {2016},\n",
    "editor = {Lee, Daniel D and Sugiyama, Masashi and von Luxburg, Ulrike and Guyon, Isabelle and Garnett, Roman},\n",
    "pages = {5076--5084},\n",
    "annote = {The idea is really simple. Regularizing hidden layers so that features in those layers form clusters.\n",
    "\n",
    "Basic problem is that, the experiments are too simple and not convincing. Not sure if it would work on more realistic cases.\n",
    "\n",
    "Some errors. at end of pp. 2, Section 3.1, the i,j mode matrix unfolding doesn't give you many concatenations of i and j mode vectors. instead, it gives a 2d slice, enumerating all indices on i and j dimensions.\n",
    "\n",
    "},\n",
    "keywords = {deep learning, regularization},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-03-14T20:49:55GMT},\n",
    "date-modified = {2017-03-29T19:25:26GMT},\n",
    "url = {http://papers.nips.cc/paper/6263-learning-deep-parsimonious-representations},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Liao/NIPS%202016%202016%20Liao.pdf},\n",
    "file = {{NIPS 2016 2016 Liao.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Liao/NIPS 2016 2016 Liao.pdf:application/pdf;NIPS 2016 2016 Liao.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Liao/NIPS 2016 2016 Liao.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/7981C07D-4431-4DF0-B3B5-91FEF3765C68}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
