{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND-OR graphs\n",
    "\n",
    "## Monograph\n",
    "\n",
    "S.-C. Zhu and D. Mumford, “A Stochastic Grammar of Images,” FNT in Computer Graphics and Vision, vol. 2, no. 4, pp. 259–362, 2006.\n",
    "\n",
    "Notes: This is the reprint version from <http://www.stat.ucla.edu/~sczhu/papers/Reprint_Grammar.pdf>\n",
    "\n",
    "It's more fancy than Alan Yuille's models as in \"Complexity of Representation and Inference in Compositional Models with Part Sharing\" or \"Semantic part segmentation using compositional model combining shape and appearance\". In that paper, there's no OR node, and maybe that makes a lot of things easier.\n",
    "\n",
    "Section 1.\n",
    "\n",
    "pp. 267 What's minimax principle?\n",
    "\n",
    "pp. 269 And-nodes are grammar rules, and Or-nodes are production rules, using some grammar terminology.\n",
    "\n",
    "pp. 270 Parse graph is a particular instance of an AND-OR graph, specifying all the OR-nodes. \n",
    "\n",
    "horizonal connections exists at some levels. when we collapse them all the way done to leaf levels, we get configuration graphs. Personally, I think 1(e) is incomplete. For example, there shuld be link between 2 and 9 as well. But whatever. Well check end of Def 5.1. Here I think it's somewhat sloppy.\n",
    "\n",
    "\n",
    "Fig 1.3 Multiple parents for one node? I think NOT.\n",
    "\n",
    "Below 1.3\n",
    "1. And-Or graph    a AND-OR structure, having (pairwise) horizontal links between AND components.\n",
    "2. parse graph.       instance of AND-OR structure, where all ORs have been specified.\n",
    "3. for each primitive, find all its ancestors with links.   create a link between two primitives if they have ancestors that are linked. Basically, reflect the link on top in the primitives.\n",
    "4. visual vocabulary. set of all instances possible for a primitive. Note that primitives can be pretty high level in concept, due to scale problem. Not sure what anchor points and open bond means here.\n",
    "5. language. everything above.\n",
    "\n",
    "Section 1.3.2 two arugments against purely unsupervised training. I think they still apply today. (2017)\n",
    "\n",
    "End of chapter 1: here they talk about advantage of AND-OR. One thing is efficient training. But I don't know how that is consistent with the fact And-or graph models are difficult to train. Maybe, here efficient training means after a whole hierarchy is modeled?\n",
    "\n",
    "\n",
    "\n",
    "Section 2.2: linking traditional grammar definitions and AND-OR graph.\n",
    "\n",
    "AND nodes are rule names, and OR nodes the the left side symbols of rules [so must be non-terminal].\n",
    "\n",
    "Section 2.3 ambiguity and reusable parts in AND-OR graph grammar for vision. In the end they assume that there's no occlusion and no sharing, so each object can be parsed as a tree structure perfectly.\n",
    "\n",
    "Section 2.4 at the end of it, it discusses potential problem of assigning probability. The example here (A->AA|a) is given in reference [10], which seems to show that using MLE, we won't get problematic estimation.\n",
    "\n",
    "Section 2.5 add some context effect during modeling that would affect the probability. That basically means horizontal connections, or some other connections not representable through tree.\n",
    "\n",
    "Address variable should be a specific concept in language modeling. For example, in Figure 2.8, the address variable for \"what\" should point to \"is\" as well as \"said\". and logically, \"what is\", and \"said what\" form \"next\" relation.\n",
    "\n",
    "What's it's essentially saying, is that in applying bigram frequency, we should not compute h(what, I), but h(said, what), and h(what, is), and they need to be determinied through some inference.\n",
    "\n",
    "Section 2.6 some special issues for vision grammar.\n",
    "1. elimination of recursive rules. I don't know how this is to be done precisely\n",
    "2. multi scale. simple, just add some termination rules.\n",
    "3. grammar is more stochastic, and it's better thought of as a mixture of strcit grammar and MRF, given the stochasticity of images.\n",
    "\n",
    "Section 2.7 previous work on image grammar. ignore, since they are probably even less popular than AND-OR graph.\n",
    "\n",
    "\n",
    "Section 3 Visual vocabulary ($\\Delta$). Here it's talking about how to model small image patches.\n",
    "\n",
    "\n",
    "Overall structure.\n",
    "\n",
    "1. First we have image primitives,\n",
    "2. then we have graphlets in 3.3, through clustering primitives.\n",
    "3. later they talk about primitives for modeling objects. See Figure. 3.7. Mathematically, they are same as image primitives, and they are composed of primitives and graphlets (Fig. 3.8 caption)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Def 3.1 is not that difficult.\n",
    "\n",
    "First $\\phi_i(x,y; \\alpha_i)$ just means that $\\alpha_i$ is the parameter for specifying one image patch, and produces a function from $(x,y)$ (pixel location) to pixel values. Some concrete examples include Gabor filter, where $\\alpha_i$ represents 6 or 7 parameters in Gabor filter, and we get a function mapping from $(x,y)$ to the intensity value of Gabor. Notice that in this case, $(x,y)$ has finite support, only concerning a small patch.\n",
    "\n",
    "$\\beta$'s definition is confusing. I think eventually in some instantiation of visual vocabulary, each $beta$ should refer to some other image word in the vocabulary.\n",
    "\n",
    "Eq. (3.2) and (3.3). A bigger image is explained by smaller pataches. Notice that some parts are not exaplainble (Eq. (3.2)).\n",
    "\n",
    "\n",
    "Section 4 horizontal relationship among parts (of potentially different levels) ($\\mathcal{R}$).\n",
    "\n",
    "According to 4.1, nodes of all levels, primitives, graphlets, parts, etc., can form relation.\n",
    "\n",
    "Def 4.1. $\\gamma$ defines the type of relationship (see Fig. 4.1), or some more patetermized description, such as those in Eq. (4.4). and $\\rho$ defines the score of this relationship given patches).\n",
    "\n",
    "Fig. 4.1 is really fancy. No idea how to train these.\n",
    "\n",
    "Notice that using Def 4.1, we only have pairwise relationships. For exmaple, in Fig 4.1. The radial example would be most natural using 5 parts, but here it's precomposed into 2 parts.\n",
    "\n",
    "Section 4.2. In what's presented here, all example configurations have nodes at the same level (primitives, parts, or objects).\n",
    "\n",
    "Section 4.3 Fig 4.3 shows the complexity of this approach. I wonder how to discriminate different configurations. The learned probablistic model would be too complicated. This approach would become intractable easily...\n",
    "\n",
    "\n",
    "Section 5 Parse graph is instantiation of an AND-OR graph. Each AND-OR graph defines a probablity distribution of parse graphs.\n",
    "\n",
    "Eq. (5.4) each and node produces a set of children nodes, as well as the relationships between the children nodes.\n",
    "\n",
    "\n",
    "> A production should also associate the open bonds of A with open bonds in C.\n",
    "\n",
    "I think it means that there's \"containing\" relationship between A and C, and they should be connected together, one end on A, one end on C.\n",
    "\n",
    "Eq. (5.6) should refer to some relationship at same level of graph.\n",
    "\n",
    "> Depending on the type of relation, there may be special rules for producing relations at a lower level from higher level relations in the collapsing process.\n",
    "\n",
    "This might be inheriting relation, as shown in Fig 1.3(d), (e).\n",
    "\n",
    "So there are two types of configurations at each level: uncollapsed (in Eq. (5.4)), and collapsed, in Eq. (5.7).\n",
    "\n",
    "pp. 321 three (or two?) features of And-or graph w.r.t. and-or tree. I think item 3 (or-node sharing) is not dealt with, or dealt with with some hack in practice. See Fig. 1.3, and description of it, above Def. 6.1. In practice, shared parts are duplicated, I guess.\n",
    "\n",
    "\n",
    "pp. 322 \"Both nodes 8 and 10 have a common ancestor node C.\" should be \"both nodes 6 abd 8\".\n",
    "\n",
    "\n",
    "Def 6.1.\n",
    "\n",
    "Here the definition for configuration, seems to be the configuration for leaf nodes. Notice that given a graph, we won't allow infinite number of recurision. So we have finite number of configurations in terms of set of variables. That's why in Eq. (6.7), you have N.\n",
    "\n",
    "Eq. (6.13) notice that you also define singular terms for intermediate and nodes as well, but in Eq. (6.13), you don't have it. So maybe some typo. Based on Section 7, I believe intermediate nodes are not there.\n",
    "\n",
    "The constraints between parts and parent, I guess, are encoded in the pairwise terms.\n",
    "\n",
    "\n",
    "\n",
    "Section 7, Section 8\n",
    "\n",
    "Take away message is: they are difficult and time-consuming, and not practical.\n",
    "\n",
    "Section 7. The algorithm for learning relationship set is not practical for me, as you assumed a large set of relationships before hand.\n",
    "\n",
    "Section 8. Based on (8.1), in the likelihood term, you can see that this likelihood function doesn't care about the unsketchable part. The notation $\\Delta_{sk}$ simply means that this term only depends on sketchable part of image, not that there's some conditional probability involved for $\\Delta_{sk}$.\n",
    "\n",
    "\n",
    "\n",
    "* p.267: As the ROC curves in Figure 9.5 shows in later section, the top-down prediction largely improves the recognition rate of the rectangles, as certain rectangles can only be hallucinated through top-down process due to occlusion and severe image degradation. -- Highlighted Feb 24, 2017\n",
    "* p.268: Thus one has to account for the transitions between instances of the same node over scales. This is the topics studied in the perceptual scale space theory [80]. -- Highlighted Feb 24, 2017\n",
    "* p.269: An And-node represents a decomposition of an entity into its parts. It corresponds to the grammar rules, for example, -- Highlighted Feb 24, 2017\n",
    "* p.269: The Or-nodes act as “switches” for alternative sub-structures, and stands for labels of classification at various levels, such as scene category, object classes, and parts etc. It corresponds to production rules like, -- Highlighted Feb 24, 2017\n",
    "* p.271: We introduce a mixed random field model [20] to represent the configurations. The mixed random field extends conventional Markov random field models by allowing address variables and handles non-local connections caused by occlusions. -- Highlighted Feb 24, 2017\n",
    "* p.271: Due to scaling property, the terminal nodes could appear at all levels of the And–Or graph. Each terminal node takes instances from certain set. The set is called a dictionary and contains image patches of various complexities. -- Highlighted Feb 24, 2017\n",
    "* p.271: Each patch is augmented with anchor points and open bond to connect with other patches. -- Highlighted Feb 24, 2017\n",
    "* p.272: For example, a car viewed in close distance is a configuration consisting of many parts and primitives. But in far distance, a car is represented by a small image patch as a whole and is not decomposable. This is a special property of the image grammar. The perceptual transition over scales is studied in [80, 84]. -- Highlighted Feb 24, 2017\n",
    "* p.272: (i) Visual learning must be guided by objectives and purposes of vision, not purely based on statistical information. -- Highlighted Feb 24, 2017\n",
    "* p.272: (ii) In almost all the unsupervised learning methods, the trainers still have to select their data carefully to contrast the involved concepts. -- Highlighted Feb 24, 2017\n",
    "* p.284: If there exists a string ω ∈ L(G) that has more than one parse tree, then G is said to be an ambiguous grammar. -- Highlighted Feb 24, 2017\n",
    "* p.287: Below we will assume that the reusable parts do not overlap so that inclusion gives us a tree-like parse structure. This simplifies immensely the computational algorithms. Future work may require dealing with diamonds more carefully (REF Geman). -- Highlighted Feb 24, 2017\n",
    "* p.288: A stochastic grammar is said to be consistent if ω∈L(G) p(ω) = 1. This is not necessarily true even when Equation (2.8) is satisfied for each non-terminal node A ∈ VN . -- Highlighted Feb 24, 2017\n",
    "* p.289: It was shown in [10] that the ML-estimation of P can rule out infinite expansion and produce a consistent grammar. -- Highlighted Feb 24, 2017\n",
    "* p.300: The conventional wavelets, Gabor image bases, image patches, and image fragments are possible examples of this visual vocabulary except that they do not have bonds. -- Highlighted Feb 24, 2017\n",
    "* p.304: Adjacent primitives are connected through their bonds. The explicit use of bonds distinguishes the image primitives from other basic image representations, such as wavelets and sparse image coding [47, 56] mentioned before, and other image patches and fragments in the recent vision literature [77]. -- Highlighted Feb 24, 2017\n",
    "* p.308: In fact, the object parts defined above are not so much different from the dictionaries of image primitives or graphlets, except that they are bigger and more structured. Indeed they form a continuous spectrum for the vision vocabulary from low to high levels of vision. -- Highlighted Feb 24, 2017\n",
    "* p.308: The inner structures of the class are encapsulated, only the bonds are visible to other classes. These bonds are used for communication between different object instances. -- Highlighted Feb 24, 2017\n",
    "* p.309: While the hierarchical visual vocabulary represents the vertical compositional structures, the relations in this section represent the horizontal links for contextual information between nodes in the hierarchy at all levels. -- Highlighted Feb 24, 2017\n",
    "* p.312: At the high level, we may find many interesting relations but each relation may only have a few occurrences in the image. -- Highlighted Feb 24, 2017\n",
    "* p.314: In summary, the image grammar which shall be presented in the next section is also called a “layered grammar.” That is, it can generate configurations as its “language” at different levels of detail. -- Highlighted Feb 24, 2017\n",
    "* p.315:  From this example, we can see that both the vertices and the bonds must be treated as random variables.  -- Highlighted Feb 24, 2017\n",
    "* p.316: It was shown that a probability model defined on such reconfigurable graphs still observes a suitable form of he Hammersley-Clifford theorem and can be simulated by Gibbs sampler. -- Highlighted Feb 24, 2017\n",
    "* p.317: In this chapter, we define parse graphs as image interpretations. Then we will show in the next chapter that these parse graphs are generated as instances by an And–Or graph. The latter is a general representation that embeds the image grammar. -- Highlighted Feb 24, 2017\n",
    "* p.318: at each level of abstraction/detail, -- Highlighted Feb 24, 2017\n",
    "* p.318: Depending on the type of relation, there may be special rules for producing relations at a lower level from higher level relations in the collapsing process. -- Highlighted Feb 24, 2017\n",
    "* p.326: The objective of this probability model is to match the frequency of parse graphs in an observed training set (supervised learning will be discussed in the next section). -- Highlighted Feb 24, 2017\n",
    "* p.326: A configuration C is assumed to be directly observable, i.e., the input, and parse graph pg are hidden variables and have to be inferred -- Highlighted Feb 24, 2017\n",
    "* p.327: Removing the 2nd and 3rd terms, this reduces to an SCFG in Equation (2.9). -- Highlighted Feb 24, 2017\n",
    "* p.327: The second term is defined on the geometric and appearance attributes of the image primitives -- Highlighted Feb 24, 2017\n",
    "* p.330: In the following we briefly discuss the first two phases. There is no significant work done for the third phase yet. -- Highlighted Feb 24, 2017\n",
    "\n",
    "### other notes for this\n",
    "\n",
    "This an excellent description of the theory for AND-OR graphs. \n",
    "\n",
    "\n",
    "#### Excellent ideas.\n",
    "\n",
    "* **Section 1.3.2 Overview of the Dataset and Learning** (pp. 272): Visual learning must be guided by objectives and purposes of vision, not purely based on statistical information. This says that some supervision and training strategy is always needed, as illustrated by the example in the end of this page: to learn the concept that a car has doors, we must select images of cars with doors **both open and closed. Otherwise the concept of door cannot be learned.** This implies that a HUGE amount of data has to be processed in order to learn visual concepts only from the statistical structure.\n",
    "\n",
    "#### Issues\n",
    "\n",
    "* In Section 1.3.1 (pp. 273), that lotus hill team is mentioned. I think such detailed annotation won't scale.\n",
    "* Fig 4.1 (pp. 311) shows some fancy relations between object parts. While they look great it's probably very difficult to learn them automatically. Thus it implies much human labor.\n",
    "* In Section 7 (pp. 330), learning is discussed. Three learning problems are presented. While first one is the simplest (since relations and dictionaries are known), second one isn't actually any more useful than the first, since still it requires a pre-determined pool (see text above Eq. (7.9), pp. 332) of relations to choose from, and third one is simply ignored, which is probably most important for the algorithm to scale.\n",
    "* Section 8 talks about inference. Since the organization of the graph is so complicated, the inference is also complicated.\n",
    "\n",
    "\n",
    "#### Things I don't understand.\n",
    "\n",
    "* In Section 1.3.1 (pp. 271), there are the concepts of parse graph and configuration. While parse graph is an instantiation of a ANR-OR graph, a configuration is produced by flattening the parse graph, and all horizontal links from the ancestors are inherited by the children. How is this inheritance defined? I guess higher nodes' (ancestors') links are actually defined in terms of its components, so that inheritance is natural by definition.\n",
    "* Section 2.7 (pp. 295) talks about other works on visual grammar. I totally don't understand them as they are probably even less popular than AND-OR graph, and the descriptions given here are very concise.\n",
    "* In Def. 6.1 (pp. 322), why containing $\\Sigma$ in the definition? Seems that $\\Sigma$ is derivable from others. Also later the definition of $\\Sigma$ (Eq 6.8) is kind of recursive, so it's somehow problematic.\n",
    "\n",
    "#### Other notes\n",
    "\n",
    "* The authors talk about reusable parts in the monograph, but I think there are actually two types of reusable parts. \n",
    "    * One is that multiple ancestors can produce same primitive. However, in an actual parse graph, only one path can be chosen.\n",
    "    * Another is that in an actual parse graph, one node becomes the child of more than one parent. This is illustrated in Fig. 2.7(c) (pp. 286). As mentioned below the Figure, in this case, they duplicate the nodes to restore the (partial, due to horizontal links) tree structure of the parse graph.\n",
    "\n",
    "~~~\n",
    "@article{Zhu:2006bu,\n",
    "author = {Zhu, Song-Chun and Mumford, David},\n",
    "title = {{A Stochastic Grammar of Images}},\n",
    "journal = {Foundations and Trends{\\textregistered} in Computer Graphics and Vision},\n",
    "year = {2006},\n",
    "volume = {2},\n",
    "number = {4},\n",
    "pages = {259--362},\n",
    "annote = {This is the reprint version from <http://www.stat.ucla.edu/{\\textasciitilde}sczhu/papers/Reprint_Grammar.pdf>\n",
    "\n",
    "It's more fancy than Alan Yuille's models as in \"Complexity of Representation and Inference in Compositional Models with Part Sharing\" or \"Semantic part segmentation using compositional model combining shape and appearance\".\n",
    "\n",
    "Section 1.\n",
    "\n",
    "pp. 267 What's minimax principle?\n",
    "\n",
    "pp. 269 And-nodes are grammar rules, and Or-nodes are production rules, using some grammar terminology.\n",
    "\n",
    "pp. 270 Parse graph is a particular instance of an AND-OR graph, specifying all the OR-nodes. \n",
    "\n",
    "horizonal connections exists at some levels. when we collapse them all the way done to leaf levels, we get configuration graphs. Personally, I think 1(e) is incomplete. For example, there shuld be link between 2 and 9 as well. But whatever. Well check end of Def 5.1. Here I think it's somewhat sloppy.\n",
    "\n",
    "\n",
    "Fig 1.3 Multiple parents for one node? I think NOT.\n",
    "\n",
    "Below 1.3\n",
    "1. And-Or graph    a AND-OR structure, having (pairwise) horizontal links between AND components.\n",
    "2. parse graph.       instance of AND-OR structure, where all ORs have been specified.\n",
    "3. for each primitive, find all its ancestors with links.   create a link between two primitives if they have ancestors that are linked. Basically, reflect the link on top in the primitives.\n",
    "4. visual vocabulary. set of all instances possible for a primitive. Note that primitives can be pretty high level in concept, due to scale problem. Not sure what anchor points and open bond means here.\n",
    "5. language. everything above.\n",
    "\n",
    "Section 1.3.2 two arugments against purely unsupervised training. I think they still apply today. (2017)\n",
    "\n",
    "End of chapter 1: here they talk about advantage of AND-OR. One thing is efficient training. But I don't know how that is consistent with the fact And-or graph models are difficult to train. Maybe, here efficient training means after a whole hierarchy is modeled?\n",
    "\n",
    "\n",
    "\n",
    "Section 2.2: linking traditional grammar definitions and AND-OR graph.\n",
    "\n",
    "AND nodes are rule names, and OR nodes the the left side symbols of rules [so must be non-terminal].\n",
    "\n",
    "Section 2.3 ambiguity and reusable parts in AND-OR graph grammar for vision. In the end they assume that there's no occlusion and no sharing, so each object can be parsed as a tree structure perfectly.\n",
    "\n",
    "Section 2.4 at the end of it, it discusses potential problem of assigning probability. The example here (A->AA|a) is given in reference [10], which seems to show that using MLE, we won't get problematic estimation.\n",
    "\n",
    "Section 2.5 add some context effect during modeling that would affect the probability. That basically means horizontal connections, or some other connections not representable through tree.\n",
    "\n",
    "Address variable should be a specific concept in language modeling. For example, in Figure 2.8, the address variable for \"what\" should point to \"is\" as well as \"said\". and logically, \"what is\", and \"said what\" form \"next\" relation.\n",
    "\n",
    "What's it's essentially saying, is that in applying bigram frequency, we should not compute h(what, I), but h(said, what), and h(what, is), and they need to be determinied through some inference.\n",
    "\n",
    "Section 2.6 some special issues for vision grammar.\n",
    "1. elimination of recursive rules. I don't know how this is to be done precisely\n",
    "2. multi scale. simple, just add some termination rules.\n",
    "3. grammar is more stochastic, and it's better thought of as a mixture of strcit grammar and MRF, given the stochasticity of images.\n",
    "\n",
    "Section 2.7 previous work on image grammar. ignore, since they are probably even less popular than AND-OR graph.\n",
    "\n",
    "\n",
    "Section 3 Visual vocabulary ($\\Delta$). Here it's talking about how to model small image patches.\n",
    "\n",
    "\n",
    "Overall structure.\n",
    "\n",
    "1. First we have image primitives,\n",
    "2. then we have graphlets in 3.3, through clustering primitives.\n",
    "3. later they talk about primitives for modeling objects. See Figure. 3.7. Mathematically, they are same as image primitives, and they are composed of primitives and graphlets (Fig. 3.8 caption)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Def 3.1 is not that difficult.\n",
    "\n",
    "First $\\phi_i(x,y; \\alpha_i)$ just means that $\\alpha_i$ is the parameter for specifying one image patch, and produces a function from $(x,y)$ (pixel location) to pixel values. Some concrete examples include Gabor filter, where $\\alpha_i$ represents 6 or 7 parameters in Gabor filter, and we get a function mapping from $(x,y)$ to the intensity value of Gabor. Notice that in this case, $(x,y)$ has finite support, only concerning a small patch.\n",
    "\n",
    "$\\beta$'s definition is confusing. I think eventually in some instantiation of visual vocabulary, each $beta$ should refer to some other image word in the vocabulary.\n",
    "\n",
    "Eq. (3.2) and (3.3). A bigger image is explained by smaller pataches. Notice that some parts are not exaplainble (Eq. (3.2)).\n",
    "\n",
    "\n",
    "Section 4 horizontal relationship among parts (of potentially different levels) ($\\mathcal{R}$).\n",
    "\n",
    "According to 4.1, nodes of all levels, primitives, graphlets, parts, etc., can form relation.\n",
    "\n",
    "Def 4.1. $\\gamma$ defines the type of relationship (see Fig. 4.1), or some more patetermized description, such as those in Eq. (4.4). and $\\rho$ defines the score of this relationship given patches).\n",
    "\n",
    "Fig. 4.1 is really fancy. No idea how to train these.\n",
    "\n",
    "Notice that using Def 4.1, we only have pairwise relationships. For exmaple, in Fig 4.1. The radial example would be most natural using 5 parts, but here it's precomposed into 2 parts.\n",
    "\n",
    "Section 4.2. In what's presented here, all example configurations have nodes at the same level (primitives, parts, or objects).\n",
    "\n",
    "Section 4.3 Fig 4.3 shows the complexity of this approach. I wonder how to discriminate different configurations. The learned probablistic model would be too complicated. This approach would become intractable easily...\n",
    "\n",
    "\n",
    "Section 5 Parse graph is instantiation of an AND-OR graph. Each AND-OR graph defines a probablity distribution of parse graphs.\n",
    "\n",
    "Eq. (5.4) each and node produces a set of children nodes, as well as the relationships between the children nodes.\n",
    "\n",
    "\n",
    "> A production should also associate the open bonds of A with open bonds in C.\n",
    "\n",
    "I think it means that there's \"containing\" relationship between A and C, and they should be connected together, one end on A, one end on C.\n",
    "\n",
    "Eq. (5.6) should refer to some relationship at same level of graph.\n",
    "\n",
    "> Depending on the type of relation, there may be special rules for producing relations at a lower level from higher level relations in the collapsing process.\n",
    "\n",
    "This might be inheriting relation, as shown in Fig 1.3(d), (e).\n",
    "\n",
    "So there are two types of configurations at each level: uncollapsed (in Eq. (5.4)), and collapsed, in Eq. (5.7).\n",
    "\n",
    "pp. 321 three (or two?) features of And-or graph w.r.t. and-or tree. I think item 3 (or-node sharing) is not dealt with, or dealt with with some hack in practice. See Fig. 1.3, and description of it, above Def. 6.1. In practice, shared parts are duplicated, I guess.\n",
    "\n",
    "\n",
    "pp. 322 \"Both nodes 8 and 10 have a common ancestor node C.\" should be \"both nodes 6 abd 8\".\n",
    "\n",
    "\n",
    "Def 6.1.\n",
    "\n",
    "Here the definition for configuration, seems to be the configuration for leaf nodes. Notice that given a graph, we won't allow infinite number of recurision. So we have finite number of configurations in terms of set of variables. That's why in Eq. (6.7), you have N.\n",
    "\n",
    "Eq. (6.13) notice that you also define singular terms for intermediate and nodes as well, but in Eq. (6.13), you don't have it. So maybe some typo. Based on Section 7, I believe intermediate nodes are not there.\n",
    "\n",
    "The constraints between parts and parent, I guess, are encoded in the pairwise terms.\n",
    "\n",
    "\n",
    "\n",
    "Section 7, Section 8\n",
    "\n",
    "Take away message is: they are difficult and time-consuming, and not practical.\n",
    "\n",
    "Section 7. The algorithm for learning relationship set is not practical for me, as you assumed a large set of relationships before hand.\n",
    "\n",
    "Section 8. Based on (8.1), in the likelihood term, you can see that this likelihood function doesn't care about the unsketchable part. The notation $\\Delta_{sk}$ simply means that this term only depends on sketchable part of image, not that there's some conditional probability involved for $\\Delta_{sk}$.\n",
    "\n",
    "},\n",
    "publisher = {Now Publishers, Inc.},\n",
    "keywords = {classics, hierarchical},\n",
    "doi = {10.1561/0600000018},\n",
    "language = {English},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-02-24T15:00:03GMT},\n",
    "date-modified = {2017-02-24T21:18:35GMT},\n",
    "abstract = {A Stochastic Grammar of Images},\n",
    "url = {http://www.nowpublishers.com/article/Details/CGV-018},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2006/Zhu/FNT%20in%20Computer%20Graphics%20and%20Vision%202006%20Zhu.pdf},\n",
    "file = {{FNT in Computer Graphics and Vision 2006 Zhu.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2006/Zhu/FNT in Computer Graphics and Vision 2006 Zhu.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1561/0600000018}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
