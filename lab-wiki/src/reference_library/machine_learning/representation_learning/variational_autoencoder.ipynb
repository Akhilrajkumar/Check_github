{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning approaches\n",
    "\n",
    "## Variational Autoencoder\n",
    "\n",
    "### original paper\n",
    "\n",
    "D. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” arXiv, vol. stat.ML, Dec. 2013.\n",
    "\n",
    "Notes: Section 2.1\n",
    "\n",
    "end of pp.2 Efficient approximate marginal inference of the variable x. this is given in Appendix D. As it says that dimension can't be higher than 5, I guess it's useless.\n",
    "\n",
    "Section 2.2\n",
    "\n",
    "All equations (1)-(3) are relatively subtle, sometimes conditioned on z, sometimes joinly x and z. But they are easy to derive. Check DL book Chapter 19.\n",
    "\n",
    "At end of this section, the \"naive\" MC gradient estimator, is the same as given in Section 20.9.1 of DL book. Notice that in this paper there's typo. It should be gradient w.r.t. theta all the times, no gradient w.r.t. q. Also, here it assumes that f(z) is constant w.r.t. theta. This means that, even if we use this naive estimator, then It will only apply to second version of SGVB (Eq. 7), not first version (Eq. 6).\n",
    "\n",
    "High variance of this estimator is also discussed in DL book. For derivation of this naive estimator, check [BJP12] in this paper, or DL book Section 20.9.1 for a related equation.\n",
    "\n",
    "Section 2.4\n",
    "\n",
    "main contribution of reparamterization trick, is that, under this formulation, a less-variance version of the gradient can be computed. By this trick, the gradient estimation becomes simple. Simply take the gradient w.r.t. theta for each sample point. Previously, if we simply get a sample of variational lower bound, then actually, there's no theta in it, and we can't take gradient.\n",
    "\n",
    "Now taking derivative become easy, as the sampling process is actually independent of theta.\n",
    "\n",
    "Check Carl Doersch's VAE tutorial for more explanation.\n",
    "\n",
    "~~~\n",
    "@article{Kingma:2013tz,\n",
    "author = {Kingma, Diederik P and Welling, Max},\n",
    "title = {{Auto-Encoding Variational Bayes}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2013},\n",
    "volume = {stat.ML},\n",
    "month = dec,\n",
    "annote = {Section 2.1\n",
    "\n",
    "end of pp.2 Efficient approximate marginal inference of the variable x. this is given in Appendix D. As it says that dimension can't be higher than 5, I guess it's useless.\n",
    "\n",
    "Section 2.2\n",
    "\n",
    "All equations (1)-(3) are relatively subtle, sometimes conditioned on z, sometimes joinly x and z. But they are easy to derive. Check DL book Chapter 19.\n",
    "\n",
    "At end of this section, the \"naive\" MC gradient estimator, is the same as given in Section 20.9.1 of DL book. Notice that in this paper there's typo. It should be gradient w.r.t. theta all the times, no gradient w.r.t. q. Also, here it assumes that f(z) is constant w.r.t. theta. This means that, even if we use this naive estimator, then It will only apply to second version of SGVB (Eq. 7), not first version (Eq. 6).\n",
    "\n",
    "High variance of this estimator is also discussed in DL book. For derivation of this naive estimator, check [BJP12] in this paper, or DL book Section 20.9.1 for a related equation.\n",
    "\n",
    "Section 2.4\n",
    "\n",
    "main contribution of reparamterization trick, is that, under this formulation, a less-variance version of the gradient can be computed. By this trick, the gradient estimation becomes simple. Simply take the gradient w.r.t. theta for each sample point. Previously, if we simply get a sample of variational lower bound, then actually, there's no theta in it, and we can't take gradient.\n",
    "\n",
    "Now taking derivative become easy, as the sampling process is actually independent of theta.\n",
    "\n",
    "Check Carl Doersch's VAE tutorial for more explanation.},\n",
    "keywords = {classics, deep learning},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-04-23T23:35:25GMT},\n",
    "date-modified = {2017-04-24T02:56:01GMT},\n",
    "url = {http://arxiv.org/abs/1312.6114},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Kingma/arXiv%202013%20Kingma.pdf},\n",
    "file = {{arXiv 2013 Kingma.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Kingma/arXiv 2013 Kingma.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/2A2193B4-61B6-419F-949E-67B773BB9F03}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "### Carl Doersch's tutorial\n",
    "\n",
    "C. Doersch, “Tutorial on Variational Autoencoders,” arXiv, vol. stat.ML, Jun. 2016.\n",
    "\n",
    "Notes: great tutorial on VAE, explaining it, as well as generative model in general, in plain words.\n",
    "\n",
    "pp. 6-7, why we can't simply estimate P(x), and then compute gradient of it.\n",
    "\n",
    "Essentially, it's very difficult to get a accurate estimate of P(x). Without much sample, our estimate of P(x) would be very bad, due to the use of l2 norm to measure reconstruction error.\n",
    "\n",
    "Section 2.1\n",
    "\n",
    "pp. 7\n",
    "\n",
    "Instead of sampling hidden variable randomly without any connection to the training data, we should try to sample from its posterior.\n",
    "\n",
    "> The key idea behind the variational autoencoder is to attempt to sample values of z that are likely to have produced X, and compute P(X) just from those. This means that we need a new function Q(z|X) which can take a value of X and give us a distribution over z values that are likely to produce X.\n",
    "\n",
    "This is exactly correct, In classical EM algorithm (such as GMM), if we know the posterior, then actually we get a tight (exact) bound on likelihood.\n",
    "\n",
    "pp. 9\n",
    "\n",
    "> Assuming we use an arbitrarily high-capacity model for Q(z|x), then Q(z|x) will hopefully actually match P(z|X), in which case this KL- divergence term will be zero, and we will be directly optimizing log P(X). As an added bonus, we have made the intractable P(z|X) tractable: we can just use Q(z|x) to compute it.\n",
    " \n",
    "not necessarily. we may also simply overfit to log P(X).\n",
    "\n",
    "\n",
    "Section 2.2\n",
    "\n",
    "pp. 10 brute force sampling without reparamterization won't give you gradient.\n",
    "\n",
    "essentially, brute force sampling can give you unbiased estimation of E[log P(X|z)], but that estimation will not give you information about its gradient w.r.t. Q.\n",
    "\n",
    "> However, in Equation 9, this dependency has disappeared!\n",
    "\n",
    "\n",
    "> Stochastic gradient descent via backpropagation can handle stochastic inputs, but not stochastic units within the network! The solution, called the “reparameterization trick” in [1], is to move the sampling to an input layer.\n",
    "\n",
    "pp. 11 what if hidden variable is discrete. \n",
    "\n",
    "I think DL book explains this better. Main problem is, by changing parameters infinitesimally in Q, we won’t change cost, and we get zero gradient almost everywhere. In those places not “almost everywhere”, we get undefined gradient (jump). therefore, we can't optimize it.\n",
    "\n",
    "\n",
    "Section 2.4.1\n",
    "\n",
    "Check appendix for a proof that with enough capacity, VAE will find true P(x). This proof assumes that the best solution can be found, and they simply prove that such solution exists.\n",
    "\n",
    "Section 2.4.2\n",
    "\n",
    "this is a very good interpretation of loss function. \n",
    "\n",
    "I don't buy that because P(X|z) doesn't have covariance, then it's inefficient. Because if our Q is perfect, then the sum of two parts is optimal (log of P(x)), and we can't reduce it anymore.\n",
    "\n",
    "\n",
    "Section 2.4.3\n",
    "\n",
    "It's talking about regularization of VAE. unlike regular AE, you can't do regualrization easily. While you have something similar to that when output is Gaussian, it will disappear for binary unit. This is because that's not regularization, but just precision in reconstruction.\n",
    "\n",
    "\n",
    "Appendix A\n",
    "\n",
    "> Note that D[Qs(z|X)||Ps(z|X)] is invariant to\n",
    "affine transformations of the sample space. \n",
    "\n",
    "I think this is from wikipedia.\n",
    "\n",
    "and here, for Q and P, they transformed sample by \n",
    "\n",
    "z’ = g(X) + (z − g(X)) ∗ σ. \n",
    "\n",
    "\n",
    "for $P_{\\sigma}$, we need to compensate the scaling with multiplying with sigma. It's the basic variable substitution in probability.\n",
    "\n",
    "~~~\n",
    "@article{Doersch:2016vb,\n",
    "author = {Doersch, Carl},\n",
    "title = {{Tutorial on Variational Autoencoders}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {stat.ML},\n",
    "month = jun,\n",
    "annote = {great tutorial on VAE, explaining it, as well as generative model in general, in plain words.\n",
    "\n",
    "pp. 6-7, why we can't simply estimate P(x), and then compute gradient of it.\n",
    "\n",
    "Essentially, it's very difficult to get a accurate estimate of P(x). Without much sample, our estimate of P(x) would be very bad, due to the use of l2 norm to measure reconstruction error.\n",
    "\n",
    "Section 2.1\n",
    "\n",
    "pp. 7\n",
    "\n",
    "Instead of sampling hidden variable randomly without any connection to the training data, we should try to sample from its posterior.\n",
    "\n",
    "> The key idea behind the variational autoencoder is to attempt to sample values of z that are likely to have produced X, and compute P(X) just from those. This means that we need a new function Q(z|X) which can take a value of X and give us a distribution over z values that are likely to produce X.\n",
    "\n",
    "This is exactly correct, In classical EM algorithm (such as GMM), if we know the posterior, then actually we get a tight (exact) bound on likelihood.\n",
    "\n",
    "pp. 9\n",
    "\n",
    "> Assuming we use an arbitrarily high-capacity model for Q(z|x), then Q(z|x) will hopefully actually match P(z|X), in which case this KL- divergence term will be zero, and we will be directly optimizing log P(X). As an added bonus, we have made the intractable P(z|X) tractable: we can just use Q(z|x) to compute it.\n",
    " \n",
    "not necessarily. we may also simply overfit to log P(X).\n",
    "\n",
    "\n",
    "Section 2.2\n",
    "\n",
    "pp. 10 brute force sampling without reparamterization won't give you gradient.\n",
    "\n",
    "essentially, brute force sampling can give you unbiased estimation of E[log P(X|z)], but that estimation will not give you information about its gradient w.r.t. Q.\n",
    "\n",
    "> However, in Equation 9, this dependency has disappeared!\n",
    "\n",
    "\n",
    "> Stochastic gradient descent via backpropagation can handle stochastic inputs, but not stochastic units within the network! The solution, called the {\\textquotedblleft}reparameterization trick{\\textquotedblright} in [1], is to move the sampling to an input layer.\n",
    "\n",
    "pp. 11 what if hidden variable is discrete. \n",
    "\n",
    "I think DL book explains this better. Main problem is, by changing parameters infinitesimally in Q, we won{\\textquoteright}t change cost, and we get zero gradient almost everywhere. In those places not {\\textquotedblleft}almost everywhere{\\textquotedblright}, we get undefined gradient (jump). therefore, we can't optimize it.\n",
    "\n",
    "\n",
    "Section 2.4.1\n",
    "\n",
    "Check appendix for a proof that with enough capacity, VAE will find true P(x). This proof assumes that the best solution can be found, and they simply prove that such solution exists.\n",
    "\n",
    "Section 2.4.2\n",
    "\n",
    "this is a very good interpretation of loss function. \n",
    "\n",
    "I don't buy that because P(X|z) doesn't have covariance, then it's inefficient. Because if our Q is perfect, then the sum of two parts is optimal (log of P(x)), and we can't reduce it anymore.\n",
    "\n",
    "\n",
    "Section 2.4.3\n",
    "\n",
    "It's talking about regularization of VAE. unlike regular AE, you can't do regualrization easily. While you have something similar to that when output is Gaussian, it will disappear for binary unit. This is because that's not regularization, but just precision in reconstruction.\n",
    "\n",
    "\n",
    "Appendix A\n",
    "\n",
    "> Note that D[Qs(z|X)||Ps(z|X)] is invariant to\n",
    "affine transformations of the sample space. \n",
    "\n",
    "I think this is from wikipedia.\n",
    "\n",
    "and here, for Q and P, they transformed sample by \n",
    "\n",
    "z{\\textquoteright} = g(X) + (z {\\textminus} g(X)) {\\textasteriskcentered} $\\sigma$. \n",
    "\n",
    "\n",
    "for $P_{\\sigma}$, we need to compensate the scaling with multiplying with sigma. It's the basic variable substitution in probability.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-04-23T23:38:17GMT},\n",
    "date-modified = {2017-04-24T02:52:20GMT},\n",
    "url = {http://arxiv.org/abs/1606.05908},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Doersch/arXiv%202016%20Doersch.pdf},\n",
    "file = {{arXiv 2016 Doersch.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Doersch/arXiv 2016 Doersch.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/0E93CF01-7F2B-4EA6-8413-E0A82F55B1A0}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "### tigher bound than variational bound\n",
    "\n",
    "Y. Burda, R. Grosse, and R. Salakhutdinov, “Importance Weighted Autoencoders,” arXiv, vol. cs.LG, Sep. 2015.\n",
    "\n",
    "Notes: essentially, replace variational lower bound with a tighter bound using importance sampling.\n",
    "\n",
    "~~~\n",
    "@article{Burda:2015ti,\n",
    "author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},\n",
    "title = {{Importance Weighted Autoencoders}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2015},\n",
    "volume = {cs.LG},\n",
    "month = sep,\n",
    "annote = {essentially, replace variational lower bound with a tighter bound using importance sampling.},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-04-24T02:52:18GMT},\n",
    "date-modified = {2017-04-24T02:55:05GMT},\n",
    "url = {http://arxiv.org/abs/1509.00519},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Burda/arXiv%202015%20Burda.pdf},\n",
    "file = {{arXiv 2015 Burda.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Burda/arXiv 2015 Burda.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/8873DBAE-C4A6-4328-BF59-46BBA4E3FAE5}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
