{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning methods\n",
    "\n",
    "## R-CNN methods\n",
    "\n",
    "### essentially a clone of R-CNN.\n",
    "\n",
    "\n",
    "B. Hariharan, P. A. Arbeláez, R. B. Girshick, and J. Malik, “Simultaneous Detection and Segmentation,” presented at the Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII, 2014, vol. 8695, pp. 297–312.\n",
    "\n",
    "Notes: RCNN on segmentation.\n",
    "\n",
    "writing is too bad. maybe just check the code (<https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/>). some terms are poorly defined, such as \"region overlap\". Also, I really don't understand if the B network has two pathways or one. Based on code, B is single-streamed.\n",
    "\n",
    "In C, not sure whether bbox-based ground-truth or region-based ground-truth are used.\n",
    "\n",
    "\n",
    "Well, I think box overlap is box IoU, and region overlap is region IoU. If > 0.5, then positive for that class; otherwise, negative (there will be a background class in the classifier, in case this region or bbox is too far from all ground-truth boxes). When the paper says \"predicting region overlap\", that means the ground-truth class is based on region IoU. Check files like <https://github.com/bharath272/sds_eccv2014/blob/master/prototxts/piwindow_train.prototxt>, indeed it has 21 class (20 VOC + 1 background).\n",
    "\n",
    "\n",
    "pp. 303 refinement.\n",
    "\n",
    "Here I believe the classifer on each of 10x10 cells are different, and they all take same input (features at 10x10 cells, as well as the downsampeld mask), but predict for different grids.\n",
    "\n",
    "Best way is to look at the code.\n",
    "\n",
    "Here, superpixel is not defined how to obtain them, but they should be smaller than the region proposals.\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.302: Concatenating these two feature vectors together gives us the feature vector we use. (In their experiments Girshick et al. found both sets of features to be useful.) -- Highlighted Feb 21, 2017\n",
    "* p.302: Therefore we change the labeling of the MCG regions to be based on segmentation overlap of the region with a ground truth region (instead of overlap with bounding box). We call this feature extractor B. -- Highlighted Feb 22, 2017\n",
    "* p.302: both A and B can be seen as instantiations of this architecture, but with different sets of weights. -- Highlighted Feb 21, 2017\n",
    "* p.302: For B, the box pathway gets its weights from a network finetuned separately using bounding box overlap, while the region pathway gets its parameters from a network finetuned separately using region overlap. -- Highlighted Feb 21, 2017\n",
    "* p.303: This training procedure corresponds to a multiple instance learning problem where each ground truth defines a positive bag of regions that overlap with it by more than 50%, and each negative region is its own bag. We found this training to work better than using just the ground truth as positives. -- Highlighted Feb 21, 2017\n",
    "* p.303: we do a strict non-max suppression using a region overlap threshold of 0. -- Highlighted Feb 21, 2017\n",
    "* p.304: Refinement uses top-down category specific information to fill in the body of the train and the cat and remove the road from the car. -- Highlighted Feb 21, 2017\n",
    "* p.304: This information needs to come from the bottom-up region candidate. Hence we train a second stage to combine this coarse mask with the region candidate. -- Highlighted Feb 22, 2017\n",
    "* p.305: A is our most naive feature extractor. It uses MCG candidates and features from the bounding box and region foreground, using a single CNN finetuned using box overlaps. -- Highlighted Feb 22, 2017\n",
    "* p.305: B is the result of finetuning a separate network exclusively on region foregrounds with labels defined by region overlap. -- Highlighted Feb 22, 2017\n",
    "\n",
    "\n",
    "~~~\n",
    "@inproceedings{Hariharan:2014dh,\n",
    "author = {Hariharan, Bharath and Arbel{\\'a}ez, Pablo Andr{\\'e}s and Girshick, Ross B and Malik, Jitendra},\n",
    "title = {{Simultaneous Detection and Segmentation}},\n",
    "booktitle = {Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII},\n",
    "year = {2014},\n",
    "editor = {Fleet, David J and Pajdla, Tom{\\'a}s and Schiele, Bernt and Tuytelaars, Tinne},\n",
    "pages = {297--312},\n",
    "publisher = {Springer},\n",
    "annote = {RCNN on segmentation.\n",
    "\n",
    "writing is too bad. maybe just check the code (<https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/>). some terms are poorly defined, such as \"region overlap\". Also, I really don't understand if the B network has two pathways or one. Based on code, B is single-streamed.\n",
    "\n",
    "In C, not sure whether bbox-based ground-truth or region-based ground-truth are used.\n",
    "\n",
    "\n",
    "Well, I think box overlap is box IoU, and region overlap is region IoU. If > 0.5, then positive for that class; otherwise, negative (there will be a background class in the classifier, in case this region or bbox is too far from all ground-truth boxes). When the paper says \"predicting region overlap\", that means the ground-truth class is based on region IoU. Check files like <https://github.com/bharath272/sds_eccv2014/blob/master/prototxts/piwindow_train.prototxt>, indeed it has 21 class (20 VOC + 1 background).\n",
    "\n",
    "\n",
    "pp. 303 refinement.\n",
    "\n",
    "Here I believe the classifer on each of 10x10 cells are different, and they all take same input (features at 10x10 cells, as well as the downsampeld mask), but predict for different grids.\n",
    "\n",
    "Best way is to look at the code.\n",
    "\n",
    "Here, superpixel is not defined how to obtain them, but they should be smaller than the region proposals.\n",
    "},\n",
    "keywords = {deep learning},\n",
    "doi = {10.1007/978-3-319-10584-0_20},\n",
    "read = {Yes},\n",
    "rating = {2},\n",
    "date-added = {2017-02-22T04:34:14GMT},\n",
    "date-modified = {2017-03-02T15:37:47GMT},\n",
    "url = {http://dx.doi.org/10.1007/978-3-319-10584-0_20},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Hariharan/ECCV%202014%20Part%20VII%202014%20Hariharan.pdf},\n",
    "file = {{ECCV 2014 Part VII 2014 Hariharan.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Hariharan/ECCV 2014 Part VII 2014 Hariharan.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1007/978-3-319-10584-0_20}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
