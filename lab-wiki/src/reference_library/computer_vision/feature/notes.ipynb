{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical features before deep learning\n",
    "\n",
    "## GIST\n",
    "\n",
    "### Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope\n",
    "\n",
    "A. Oliva and A. Torralba, “Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope,” IJCV, vol. 42, no. 3, pp. 145–175, 2001.\n",
    "\n",
    "Classical paper about GIST.\n",
    "\n",
    "However, how GIST descriptor is computed is not given in the paper. Instead, this paper tries to learn some directions in the DFT (or windowed DFT) of an image, representing some \"spatial envelope properties\" that human can understand, such as roughedness, openness, etc. This is why spatial envelop properties are better than PCA, as said in pp. 154. \n",
    "\n",
    "> However, the contribution of each feature cannot be understood as they stand, and more importantly, they are not directly meaningful to human observers.\n",
    "\n",
    "To find those spatial envelope directions (say, direction discriminating openness), they need label of openness, say, ranging from -1 to 1. But I think this is pretty subjective. Then solve the regression problem using Eq. (12).\n",
    "\n",
    "I feel that what people call GIST descriptor (in their website <http://people.csail.mit.edu/torralba/code/spatialenvelope/>) might be an approximation of doing PCA on spectrogram (WFT in the paper). As here we only need features, then whether it's interpretable to human is less concerned.\n",
    "\n",
    "For a descrption of GIST code, see <https://www.quora.com/Computer-Vision-What-is-a-GIST-descriptor>.\n",
    "\n",
    "> Given an input image, a GIST descriptor is computed by\n",
    "> 1. Convolve the image with 32 Gabor filters at 4 scales, 8 orientations, producing 32 feature maps of the same size > of the input image.\n",
    "> 2. Divide each feature map into 16 regions (by a 4x4 grid), and then average the feature values within each region.\n",
    "> 3. Concatenate the 16 averaged values of all 32 feature maps, resulting in a 16x32=512 GIST descriptor.\n",
    "> Intuitively, GIST summarizes the gradient information (scales and orientations) for different parts of an image, which provides a rough description (the gist) of the scene.\n",
    "\n",
    "or to be more succint, see 2.1 of Evaluation of GIST descriptors for web-scale image search (DOI 10.1145/1646396.1646421)\n",
    "\n",
    "> To compute the color GIST description the image is segmented by a 4 by 4 grid for which orientation histograms are extracted.\n",
    "\n",
    "or there's a picture from <http://graphics.cs.cmu.edu/courses/15-463/2012_fall/Lectures/InternetData1.ppt>\n",
    "\n",
    "![](./_gist/gist_cmu.png)\n",
    "\n",
    "Below Eq. (12), it mentions the case where the attribute of spatial envelope is binary. This is also mentioned in PRML when discussing LDA.\n",
    "\n",
    "in pp. 155, I don't understand why they need to have positive and negative parts. Maybe only in this way can you get the filters in the spatial domain. Anyway, it's just a visualization technique.\n",
    "\n",
    "end of pp. 151. unlocalized (energy spectra) and localized (spectrogram). So energy spectra only has frequency, and spectrogram has position/time x freqency.\n",
    "\n",
    "Other notes\n",
    "\n",
    "* p.154: However, the contribution of each feature cannot be understood as they stand, and more importantly, they are not directly meaningful to human observers. -- Highlighted Feb 15, 2017\n",
    "* p.155: The functions h+ and h− are not uniquely constrained by the DST as the phase function can have any value. We fix the phase function at zero in order to have localized spatial functions. -- Highlighted Feb 15, 2017\n",
    "* p.155: In such a case, the regression parameters (Eq. (12)) are equivalent to the parameters obtained by applying a linear discriminant analysis (see Ripley, 1996; Swets and Weng, 1996). -- Highlighted Feb 15, 2017\n",
    "\n",
    "~~~\n",
    "@article{Oliva:2001ck,\n",
    "author = {Oliva, Aude and Torralba, Antonio},\n",
    "title = {{Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope}},\n",
    "journal = {International Journal of Computer Vision},\n",
    "year = {2001},\n",
    "volume = {42},\n",
    "number = {3},\n",
    "pages = {145--175},\n",
    "annote = {Classical paper about GIST paper.\n",
    "\n",
    "However, how GIST descriptor is computed is not given in the paper. Instead, this paper tries to learn some directions in the DFT (or windowed DFT) of an image, representing some \"spatial envelope properties\" that human can understand, such as roughedness, openness, etc. This is why spatial envelop properties are better than PCA, as said in pp. 154. \n",
    "\n",
    "However, the con- tribution of each feature cannot be understood as they stand, and more importantly, they are not directly mean- ingful to human observers.\n",
    "\n",
    "To find those spatial envelope directions (say, direction discriminating openness), they need label of openness, say, ranging from -1 to 1. But I think this is pretty subjective. Then solve the regression problem using Eq. (12).\n",
    "\n",
    "I feel that what people call GIST descriptor (in their website <http://people.csail.mit.edu/torralba/code/spatialenvelope/>) might be an approximation of doing PCA on spectrogram (WFT). As here we only need features, then whether it's interpretable to human is less concerned.\n",
    "\n",
    "For a descrption of GIST code, see <https://www.quora.com/Computer-Vision-What-is-a-GIST-descriptor>.\n",
    "\n",
    "========START========\n",
    "Given an input image, a GIST descriptor is computed by\n",
    "1. Convolve the image with 32 Gabor filters at 4 scales, 8 orientations, producing 32 feature maps of the same size of the input image.\n",
    "2. Divide each feature map into 16 regions (by a 4x4 grid), and then average the feature values within each region.\n",
    "3. Concatenate the 16 averaged values of all 32 feature maps, resulting in a 16x32=512 GIST descriptor.\n",
    "Intuitively, GIST summarizes the gradient information (scales and orientations) for different parts of an image, which provides a rough description (the gist) of the scene.\n",
    "========END========\n",
    "\n",
    "or to be more succint, see 2.1 of Evaluation of GIST descriptors for web-scale image search (DOI 10.1145/1646396.1646421)\n",
    "\n",
    "========START=========\n",
    "To compute the color GIST description the image is segmented by a 4 by 4 grid for which orientation histograms are extracted.\n",
    "========END=======\n",
    "\n",
    "\n",
    "\n",
    "Below Eq. (12), it mentions the case where the attribute of spatial envelope is binary. This is also mentioned in PRML when discussing LDA.\n",
    "\n",
    "\n",
    "in pp. 155, I don't understand why they need to have positive and negative parts. Maybe only in this way can you get the filters in the spatial domain. Anyway, it's just a visualization technique.\n",
    "\n",
    "end of pp. 151. unlocalized (energy spectra) and localized (spectrogram). So energy spectra only has frequency, and spectrogram has position/time x freqency.},\n",
    "publisher = {Kluwer Academic Publishers},\n",
    "keywords = {classics},\n",
    "doi = {10.1023/A:1011139631724},\n",
    "language = {English},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-02-15T18:42:13GMT},\n",
    "date-modified = {2017-02-17T16:15:52GMT},\n",
    "abstract = {In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a},\n",
    "url = {http://link.springer.com/article/10.1023/A:1011139631724},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2001/Oliva/IJCV%202001%20Oliva.pdf},\n",
    "file = {{IJCV 2001 Oliva.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2001/Oliva/IJCV 2001 Oliva.pdf:application/pdf;IJCV 2001 Oliva.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2001/Oliva/IJCV 2001 Oliva.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1023/A:1011139631724}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature benchmarks, comparisons\n",
    "\n",
    "## good old features\n",
    "\n",
    "### Benchmark of feature encoding methods for image classification\n",
    "\n",
    "K. Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisserman, “The devil is in the details: an evaluation of recent feature encoding methods,” presented at the British Machine Vision Conference 2011, 2011, pp. 76.1–76.12.\n",
    "\n",
    "Their toolkit <http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/> might be useful for evaluating unsupervised models in general.\n",
    "\n",
    "* p.4: where s is a constant chosen to balance sk with uk numerically -- Highlighted Feb 17, 2017\n",
    "* p.5: In the experiments the spatial regions are obtained by dividing the image in 1 × 1, 3 × 1 (three horizontal stripes), and 2 × 2 (four quadrants) grids, for a total of 8 regions -- Highlighted Feb 17, 2017\n",
    "* p.7: However, as we learned from personal communication with the authors, [27] used nontrivial modifications not discussed in the paper to achieve those results (these include using LDA to compute the SVM kernel and second order information as in the Fisher encoding). According to the authors, the performance achieved with our implementation is representative of their method, given that we did not apply these additional modifications. -- Highlighted Feb 16, 2017\n",
    "\n",
    "~~~\n",
    "@inproceedings{Chatfield:2011ks,\n",
    "author = {Chatfield, Ken and Lempitsky, Victor and Vedaldi, Andrea and Zisserman, Andrew},\n",
    "title = {{The devil is in the details: an evaluation of recent feature encoding methods}},\n",
    "booktitle = {British Machine Vision Conference 2011},\n",
    "year = {2011},\n",
    "pages = {76.1--76.12},\n",
    "publisher = {British Machine Vision Association},\n",
    "annote = {Their toolkit <http://www.robots.ox.ac.uk/{\\textasciitilde}vgg/research/encoding_eval/> might be useful for evaluating unsupervised models in general.},\n",
    "keywords = {benchmark},\n",
    "doi = {10.5244/C.25.76},\n",
    "isbn = {1-901725-43-X},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-02-15T22:30:15GMT},\n",
    "date-modified = {2017-02-17T19:45:18GMT},\n",
    "url = {http://www.bmva.org/bmvc/2011/proceedings/paper76/index.html},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2011/Chatfield/BMVC%202011%202011%20Chatfield.pdf},\n",
    "file = {{BMVC 2011 2011 Chatfield.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2011/Chatfield/BMVC 2011 2011 Chatfield.pdf:application/pdf;BMVC 2011 2011 Chatfield.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2011/Chatfield/BMVC 2011 2011 Chatfield.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.5244/C.25.76}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## CNN features rule them all.\n",
    "\n",
    "K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the Devil in the Details: Delving Deep into Convolutional Nets,” arXiv, vol. cs.CV, May 2014.\n",
    "\n",
    "Notes: Essentially, CNN features are the best, though CNN training tricks, such as data augmentation, etc. may help shallow features, but the differences are big.\n",
    "\n",
    "~~~\n",
    "@article{Chatfield:2014ww,\n",
    "author = {Chatfield, Ken and Simonyan, K and Vedaldi, A and Zisserman, Andrew},\n",
    "title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2014},\n",
    "volume = {cs.CV},\n",
    "month = may,\n",
    "annote = {Essentially, CNN features are the best, though CNN training tricks, such as data augmentation, etc. may help shallow features, but the differences are big.},\n",
    "keywords = {benchmark, deep learning},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-02-28T18:41:04GMT},\n",
    "date-modified = {2017-03-27T01:00:00GMT},\n",
    "url = {http://arxiv.org/abs/1405.3531},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Chatfield/arXiv%202014%20Chatfield.pdf},\n",
    "file = {{arXiv 2014 Chatfield.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Chatfield/arXiv 2014 Chatfield.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/59D43402-21F4-4016-8F5B-E5D4C6913567}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning features\n",
    "\n",
    "## DeCAF (I guess it's just AlexNet)\n",
    "\n",
    "J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, “DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,” presented at the Proceedings of the th International Conference on Machine Learning, ICML , Beijing, China, - June, 2014, vol. 32, pp. 647–655.\n",
    "\n",
    "Notes: maybe most influential paper showing usefulness of CNN in many tasks. It's famous because it's the predecessor of Caffe.\n",
    "\n",
    "DeCAF just means the feature vector of CNN, say fc7.\n",
    "\n",
    "pp. 2 has the main result:\n",
    "\n",
    "\n",
    "Our main result is the empirical validation that a generic visual feature based on a convolutional network weights trained on ImageNet outperforms a host of conventional vi- sual representations on standard benchmark object recog- nition tasks\n",
    "\n",
    "\n",
    "Section 3.1\n",
    "\n",
    "They use a AlexNet model trained by themselves, getting 42.9 error rate. It's same as blvc_alexnet of Caffe. See <https://github.com/BVLC/caffe/tree/rc4/models/bvlc_alexnet>\n",
    "\n",
    "I think in practice they use 227x227 input, as indicated by the decaf github project's wiki <https://github.com/UCBAIR/decaf-release/wiki/imagenet>\n",
    "\n",
    "as well as code in <https://github.com/UCBAIR/decaf-release/blob/6fa4cdfbd0d0b8d486d7146bf1e32edd3662fec4/decaf/scripts/imagenet.py>\n",
    "\n",
    "For 224 vs 227 they think it's just some trick to speed up GPU computation. See <https://github.com/UCBAIR/decaf-release/wiki/imagenet>\n",
    "\n",
    "\"The Decaf implementation uses input images of size 227x227, while the cuda-convnet code uses images of size 224x224. We did 227x227 simply to have a full convolution (if the size is 224x224, the last row/column will only have height/width 8 instead of 11). We believe that cuda-convnet chose 224 for speed consideration as that creates good performance for GPUs. The performance difference should not be big.\n",
    "\n",
    "Since we trained our network using GPU and are running on CPU, we actually observed some performance differences between them. We are not clear yet what caused it (it might be a bug in our code, admittedly).\n",
    "\"\n",
    "\n",
    "\n",
    "pp. 4 see footnote 5. In order for tSNE to work, some random projection might be needed.\n",
    "\n",
    "pp. 4 Figure 3 shows that fully connected layer takes most of time. Wonder if that still applies.\n",
    "\n",
    "pp. 6 4.3 The Deformable Part descriptors sounds like averaging features from different parts, with some learned weights. But anyway, this is not important.\n",
    "\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.648: A key question for such learning problems is to find a feature represen-tation that captures the object category related information while discarding noise irrelevant to object category information such as illumination. -- Highlighted Feb 14, 2017\n",
    "* p.650: in large networks such as the current ImageNet CNN model, the last few fully-connected layers require the most computation time as they involve large transform matrices. This is particularly important when one considers classification into a larger number of categories or with larger hidden-layer sizes, suggesting that certain sparse approaches such as Bayesian output coding (Hsu et al., 2009) may be necessary to carry out classification into even larger number of object categories. -- Highlighted Feb 17, 2017\n",
    "* p.650: Some of the features were very high dimensional (e.g. LLC had 16K dimension), in which case we preprocess them by randomly projecting them down to 512 dimensions – random projections are cheap to apply and tend to preserve distances well, which is all the t-SNE algorithm cares about. -- Highlighted Feb 17, 2017\n",
    "\n",
    "~~~\n",
    "@inproceedings{Donahue:2014ta,\n",
    "author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},\n",
    "title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},\n",
    "booktitle = {Proceedings of the th International Conference on Machine Learning, ICML , Beijing, China, - June },\n",
    "year = {2014},\n",
    "pages = {647--655},\n",
    "publisher = {JMLR.org},\n",
    "annote = {maybe most influential paper showing usefulness of CNN in many tasks. It's famous because it's the predecessor of Caffe.\n",
    "\n",
    "DeCAF just means the feature vector of CNN, say fc7.\n",
    "\n",
    "pp. 2 has the main result:\n",
    "\n",
    "\n",
    "Our main result is the empirical validation that a generic visual feature based on a convolutional network weights trained on ImageNet outperforms a host of conventional vi- sual representations on standard benchmark object recog- nition tasks\n",
    "\n",
    "\n",
    "Section 3.1\n",
    "\n",
    "They use a AlexNet model trained by themselves, getting 42.9 error rate. It's same as blvc_alexnet of Caffe. See <https://github.com/BVLC/caffe/tree/rc4/models/bvlc_alexnet>\n",
    "\n",
    "I think in practice they use 227x227 input, as indicated by the decaf github project's wiki <https://github.com/UCBAIR/decaf-release/wiki/imagenet>\n",
    "\n",
    "as well as code in <https://github.com/UCBAIR/decaf-release/blob/6fa4cdfbd0d0b8d486d7146bf1e32edd3662fec4/decaf/scripts/imagenet.py>\n",
    "\n",
    "For 224 vs 227 they think it's just some trick to speed up GPU computation. See <https://github.com/UCBAIR/decaf-release/wiki/imagenet>\n",
    "\n",
    "\"The Decaf implementation uses input images of size 227x227, while the cuda-convnet code uses images of size 224x224. We did 227x227 simply to have a full convolution (if the size is 224x224, the last row/column will only have height/width 8 instead of 11). We believe that cuda-convnet chose 224 for speed consideration as that creates good performance for GPUs. The performance difference should not be big.\n",
    "\n",
    "Since we trained our network using GPU and are running on CPU, we actually observed some performance differences between them. We are not clear yet what caused it (it might be a bug in our code, admittedly).\n",
    "\"\n",
    "\n",
    "\n",
    "pp. 4 see footnote 5. In order for tSNE to work, some random projection might be needed.\n",
    "\n",
    "pp. 4 Figure 3 shows that fully connected layer takes most of time. Wonder if that still applies.\n",
    "\n",
    "pp. 6 4.3 The Deformable Part descriptors sounds like averaging features from different parts, with some learned weights. But anyway, this is not important.\n",
    "\n",
    "},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-14T20:38:53GMT},\n",
    "date-modified = {2017-02-17T20:55:28GMT},\n",
    "url = {http://jmlr.org/proceedings/papers/v32/donahue14.html},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Donahue/ICML%202014%202014%20Donahue.pdf},\n",
    "file = {{ICML 2014 2014 Donahue.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Donahue/ICML 2014 2014 Donahue.pdf:application/pdf;ICML 2014 2014 Donahue.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Donahue/ICML 2014 2014 Donahue.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/F1F8335F-D616-413E-B778-A41B7EB95AB5}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## Hypercolumn for fine-grained tasks\n",
    "\n",
    "B. Hariharan, P. Arbelaez, R. B. Girshick, and J. Malik, “Hypercolumns for object segmentation and fine-grained localization,” presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 447–456.\n",
    "\n",
    "\n",
    "Hypercolumn, multilayer representation is going to help fine-grained tasks.\n",
    "\n",
    "pp. 449 Interpolating into a grid of classifiers\n",
    "\n",
    "based on description here, SDS paper always uses separate classifiers at each of 10x10 grid.\n",
    "\n",
    "Notice that here it seems they use classifers across different object categories, probably because hypercolumn vector already provides some class info, and that could be used to adapt one classifier at one location to different classes.\n",
    "\n",
    "pp. 450 Efficient classification using convolutions and upsampling\n",
    "\n",
    "Essentially, they say they can swap order of upsampling for higher layers and linear classifier. I think here they will only deal with upsampling, as they first all resize input region to 50x50, and I think number of columns in each feature map can only be smaller than 50x50.\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.447: We borrow the term “hypercolumn” from neuroscience, where it is used to describe a set of V1 neurons sensitive to edges at multiple orientations and multiple frequencies arranged in a columnar structure [24]. However, our hypercolumn includes not just edge detectors but also more semantic units and is thus a more general notion. -- Highlighted Feb 22, 2017\n",
    "\n",
    "~~~\n",
    "@inproceedings{Hariharan:2015ig,\n",
    "author = {Hariharan, Bharath and Arbelaez, Pablo and Girshick, Ross B and Malik, Jitendra},\n",
    "title = {{Hypercolumns for object segmentation and fine-grained localization}},\n",
    "booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "year = {2015},\n",
    "pages = {447--456},\n",
    "publisher = {IEEE},\n",
    "annote = {Hypercolumn, multilayer representation is going to help fine-grained tasks.\n",
    "\n",
    "\n",
    "pp. 449 Interpolating into a grid of classifiers\n",
    "\n",
    "based on description here, SDS paper always uses separate classifiers at each of 10x10 grid.\n",
    "\n",
    "Notice that here it seems they use classifers across different object categories, probably because hypercolumn vector already provides some class info, and that could be used to adapt one classifier at one location to different classes.\n",
    "\n",
    "pp. 450 Efficient classification using convolutions and upsampling\n",
    "\n",
    "Essentially, they say they can swap order of upsampling for higher layers and linear classifier. I think here they will only deal with upsampling, as they first all resize input region to 50x50, and I think number of columns in each feature map can only be smaller than 50x50.\n",
    "\n",
    "},\n",
    "keywords = {deep learning, hierarchical},\n",
    "doi = {10.1109/CVPR.2015.7298642},\n",
    "isbn = {978-1-4673-6964-0},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-02-20T19:09:40GMT},\n",
    "date-modified = {2017-02-22T14:37:14GMT},\n",
    "url = {http://ieeexplore.ieee.org/document/7298642/},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Hariharan/CVPR%202015%202015%20Hariharan.pdf},\n",
    "file = {{CVPR 2015 2015 Hariharan.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Hariharan/CVPR 2015 2015 Hariharan.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1109/CVPR.2015.7298642}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## population codes at higher layers can act as part detectors\n",
    "\n",
    "J. Wang, Z. Zhang, C. Xie, V. Premachandran, and A. L. Yuille, “Unsupervised learning of object semantic parts from internal states of CNNs by population encoding,” arXiv, vol. cs.LG, Nov. 2015.\n",
    "\n",
    "Notes: Essentially, they found part detectors in population code (visual concept) of higher layers of CNN.\n",
    "\n",
    "Main problem of this is that they haven't proposed ways to train network so that more VCs can be learned. This is essentially addressed in paper \"Learning Deep Parsimonious Representations\". by Raquel Urtasun\n",
    "\n",
    "\n",
    "in Section 6.4, they show that each VC may correspond to multiple semantic parts as defiend by human, as vice versa. This is understandable.\n",
    "\n",
    "This is proved by Figure 7, where they consider each VC as a multi part detector. They also tried combining VCs for one part would increase performance as well (well not sure how it's implemented, maybe by mean over scores, max over scores, or average of different VCs, etc.) But anyway, key point is that VC and parts are not one-to-one.\n",
    "\n",
    "\n",
    "In the end, they say future work is to work on compositional model (citation [24]). But I think isn't fc layers already doing that?\n",
    "\n",
    "other notes\n",
    "* p.1: Note, we use the term “semantic parts” to mean object parts, like wheels and windows of cars, which are defined in terms of the three-dimensional object. -- Highlighted Mar 28, 2017\n",
    "\n",
    "~~~\n",
    "@article{Wang:2015wd,\n",
    "author = {Wang, J and Zhang, Z and Xie, C and Premachandran, V and Yuille, Alan L},\n",
    "title = {{Unsupervised learning of object semantic parts from internal states of CNNs by population encoding}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2015},\n",
    "volume = {cs.LG},\n",
    "month = nov,\n",
    "annote = {Essentially, they found part detectors in population code (visual concept) of higher layers of CNN.\n",
    "\n",
    "Main problem of this is that they haven't proposed ways to train network so that more VCs can be learned. This is essentially addressed in paper \"Learning Deep Parsimonious Representations\". by Raquel Urtasun\n",
    "\n",
    "\n",
    "in Section 6.4, they show that each VC may correspond to multiple semantic parts as defiend by human, as vice versa. This is understandable.\n",
    "\n",
    "This is proved by Figure 7, where they consider each VC as a multi part detector. They also tried combining VCs for one part would increase performance as well (well not sure how it's implemented, maybe by mean over scores, max over scores, or average of different VCs, etc.) But anyway, key point is that VC and parts are not one-to-one.\n",
    "\n",
    "\n",
    "In the end, they say future work is to work on compositional model (citation [24]). But I think isn't fc layers already doing that?\n",
    "},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-03-27T21:52:29GMT},\n",
    "date-modified = {2017-03-28T19:44:27GMT},\n",
    "url = {http://arxiv.org/abs/1511.06855},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Wang/arXiv%202015%20Wang.pdf},\n",
    "file = {{arXiv 2015 Wang.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Wang/arXiv 2015 Wang.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/57BC5B33-8860-4F56-BB50-7296A5877A19}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## CNN + Spatial Pyramid\n",
    "\n",
    "Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale Orderless Pooling of Deep Convolutional Activation Features,” presented at the Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII, 2014, vol. 8695, pp. 392–407.\n",
    "\n",
    "Notes: Essentially, it's a Spatial Pyramid Matching method, using FC7 features....\n",
    "\n",
    "In Section 3, they argued why FC7 has some spatial specificity. I think this is consistent with later works, such as \"Understanding Deep Image Representations by Inverting Them\".\n",
    "\n",
    "~~~\n",
    "@inproceedings{Gong:2014jk,\n",
    "author = {Gong, Yunchao and Wang, Liwei and Guo, Ruiqi and Lazebnik, Svetlana},\n",
    "title = {{Multi-scale Orderless Pooling of Deep Convolutional Activation Features}},\n",
    "booktitle = {Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII},\n",
    "year = {2014},\n",
    "editor = {Fleet, David J and Pajdla, Tom{\\'a}s and Schiele, Bernt and Tuytelaars, Tinne},\n",
    "pages = {392--407},\n",
    "publisher = {Springer},\n",
    "annote = {Essentially, it's a Spatial Pyramid Matching method, using FC7 features....\n",
    "\n",
    "In Section 3, they argued why FC7 has some spatial specificity. I think this is consistent with later works, such as \"Understanding Deep Image Representations by Inverting Them\".},\n",
    "keywords = {deep learning},\n",
    "doi = {10.1007/978-3-319-10584-0_26},\n",
    "language = {English},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-05-05T20:06:51GMT},\n",
    "date-modified = {2017-05-05T20:42:11GMT},\n",
    "abstract = {Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustnes},\n",
    "url = {http://dx.doi.org/10.1007/978-3-319-10584-0_26},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Gong/ECCV%202014%20Part%20VII%202014%20Gong.pdf},\n",
    "file = {{ECCV 2014 Part VII 2014 Gong.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Gong/ECCV 2014 Part VII 2014 Gong.pdf:application/pdf;ECCV 2014 Part VII 2014 Gong.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Gong/ECCV 2014 Part VII 2014 Gong.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1007/978-3-319-10584-0_26}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
