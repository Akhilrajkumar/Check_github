{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "## CNN methods, not Deep Learning (AlexNet or more powerful models)\n",
    "\n",
    "### Recurrent CNN to handle space\n",
    "\n",
    "P. H. O. Pinheiro and R. Collobert, “Recurrent Convolutional Neural Networks for Scene Labeling,” presented at the Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, 2014, vol. 32, pp. 82–90.\n",
    "\n",
    "Notes: Composition of CNN, as an RNN, for scene segmentation. You can think of this as another type of parameter sharing. The idea is cool, maybe worthing exploring given current state-of-the-art frameworks.\n",
    "\n",
    "Above Section 4, they talk about solving the problem of downsampling. Essentially the same ieda as in OverFeat, to increase resolution.\n",
    "\n",
    "The composition can be considered as deeper cnn with bigger RFs. During training, they have to run the network across multiple places (the bigger the p, the more, and also more padding) to get label at all places.\n",
    "\n",
    "Notice that during training, they simply use random patches; so they don't have to think about padding too much. \n",
    "\n",
    "Table 5 is just showing how this dense interleaving of convolution will help performance. For those with 1/8, I guess they simply upsample everything.\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.82: The image labeling problem is most commonly addressed with some kind of local classifier constrained in its predictions with a graphical model -- Highlighted Feb 20, 2017\n",
    "* p.83: Compared to existing approaches, our method does not rely on any task-specific feature (see Table 1). Furthermore, our scene labeling system is able to extract relevant contextual information from raw pixels. -- Highlighted Feb 20, 2017\n",
    "* p.84: During the training phase, the size of the input patch Ii,j,k is chosen carefully such that the output layers produces 1 × 1 planes, which are then interpreted as scores for each class of interest. -- Highlighted Feb 20, 2017\n",
    "* p.85: an input “image” Fp of N + 3 features maps -- Highlighted Feb 20, 2017\n",
    "* p.85: Yet another approach would be the use of a multiscale convolutional network (Farabet et al., 2013). Large contexts are integrated into local decisions while making the model still manageable in terms of parameters/dimensionality. Label coherence can then be increased by leveraging, for instance, superpixels. -- Highlighted Feb 20, 2017\n",
    "* p.85: 2Ip is Ii,j,k scaled to the size of f(Fp−1). -- Highlighted Feb 20, 2017\n",
    "* p.86: In fact, it is possible to compute efficiently the label plane with a fine resolution by feeding to the network several versions of the input image, shifted on the X and Y axis. Figure 3 shows an example for a network which would have only one 2 × 2 pooling layer, and one output plane: low resolution label planes (coming out of the network for the input image shifted by (0, 0), (0, 1), (1, 0) and (1, 1) pixels) are “merged” to form the high resolution label plane. -- Highlighted Feb 20, 2017\n",
    "* p.86: In all three cases, the architecture produces labels (1 × 1 output planes) corresponding to the pixel at the center of the input patch. -- Highlighted Feb 20, 2017\n",
    "* p.88: As shown in Figure 2, the input context patch size depends directly on the number of network instances in the recurrent architecture. In the case of rCNN2, the input patch size is 25 × 25 when considering one instance (f) and 121 × 121 when considering two network instances (f ◦ f ). -- Highlighted Feb 20, 2017\n",
    "* p.88: The input patch size is 23 × 23, 67 × 67 and 155 × 155 when considering one, two or three instances of the network (f, f ◦ f and f ◦ f ◦ f), respectively -- Highlighted Feb 20, 2017\n",
    "\n",
    "~~~\n",
    "@inproceedings{Pinheiro:2014uk,\n",
    "author = {Pinheiro, Pedro H O and Collobert, Ronan},\n",
    "title = {{Recurrent Convolutional Neural Networks for Scene Labeling}},\n",
    "booktitle = {Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014},\n",
    "year = {2014},\n",
    "pages = {82--90},\n",
    "publisher = {JMLR.org},\n",
    "annote = {Composition of CNN, as an RNN, for scene segmentation. You can think of this as another type of parameter sharing. The idea is cool, maybe worthing exploring given current state-of-the-art frameworks.\n",
    "\n",
    "Above Section 4, they talk about solving the problem of downsampling. Essentially the same ieda as in OverFeat, to increase resolution.\n",
    "\n",
    "The composition can be considered as deeper cnn with bigger RFs. During training, they have to run the network across multiple places (the bigger the p, the more, and also more padding) to get label at all places.\n",
    "\n",
    "Notice that during training, they simply use random patches; so they don't have to think about padding too much. \n",
    "\n",
    "Table 5 is just showing how this dense interleaving of convolution will help performance. For those with 1/8, I guess they simply upsample everything.},\n",
    "keywords = {deep learning, recurrent},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-02-20T20:02:14GMT},\n",
    "date-modified = {2017-02-20T21:45:01GMT},\n",
    "url = {http://jmlr.org/proceedings/papers/v32/pinheiro14.html},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Pinheiro/ICML%202014%202014%20Pinheiro.pdf},\n",
    "file = {{ICML 2014 2014 Pinheiro.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Pinheiro/ICML 2014 2014 Pinheiro.pdf:application/pdf;ICML 2014 2014 Pinheiro.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2014/Pinheiro/ICML 2014 2014 Pinheiro.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/66E16CAE-B0B9-48DB-B0C5-AAC08EDF6C7F}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "### Multiresolution CNN\n",
    "\n",
    "C. Farabet, C. Couprie, L. Najman, and Y. LeCun, “Learning Hierarchical Features for Scene Labeling,” TPAMI, vol. 35, no. 8, pp. 1915–1929, 2013.\n",
    "\n",
    "Notes: Good point: hierarchy, multi resolution\n",
    "Bad point: complicated training procedure.\n",
    "\n",
    "\n",
    "\n",
    "This tree cover method is actually faster than CRF, as it simply involves finding best $C_k$ for each pixel, without caring about relationships between pixels. The set of C_k found is non-disjoint. See Fig 5's caption to understand it better.\n",
    "\n",
    "\n",
    "The segmentation method used are not consistent. Sometimes they use gPb, a tree method, sometimes they produce multiple levels, using felzenszwalb and huttenlocher. But anyway.\n",
    "\n",
    "\n",
    "\n",
    "pp. 1\n",
    "\n",
    "> A striking characteristic of the system proposed here is that the use of a large contextual window to label pixels reduces the requirement for sophisticated postprocessing methods that ensure the consistency of the labeling.\n",
    "\n",
    "However, I think postprocessing is sophisticated here as well\n",
    "\n",
    "Section 4.1 Superpixel method\n",
    "\n",
    "They trained a classifier in superpixel method. Maybe this is better than using classifer from the per-pixel classifer.\n",
    "\n",
    "Seciton 4.2 CRF method\n",
    "\n",
    "Eq. (15) since i and j are neighbors, using graideint of i or j in energy function shouldn't matter.\n",
    "\n",
    "Section 4.3.2 Cover method.\n",
    "\n",
    "To compute S, You always first restrict feature map vectors at locations covered by $C_k$, and then do spatial pyramid pooling with 3x3 (See Fig 6 caption) bins. Then we can predict its class to get purity.\n",
    "\n",
    "other notes\n",
    "\n",
    "> p.1915: A striking characteristic of the system proposed here is that the use of a large contextual window to label pixels reduces the requirement for sophisticated postprocessing methods that ensure the consistency of the labeling. -- Highlighted Feb 21, 2017\n",
    "\n",
    "~~~\n",
    "@article{Farabet:2013eu,\n",
    "author = {Farabet, Cl{\\'e}ment and Couprie, Camille and Najman, Laurent and LeCun, Yann},\n",
    "title = {{Learning Hierarchical Features for Scene Labeling}},\n",
    "journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n",
    "year = {2013},\n",
    "volume = {35},\n",
    "number = {8},\n",
    "pages = {1915--1929},\n",
    "annote = {Good point: hierarchy, multi resolution\n",
    "Bad point: complicated training procedure.\n",
    "\n",
    "\n",
    "\n",
    "This tree cover method is actually faster than CRF, as it simply involves finding best $C_k$ for each pixel, without caring about relationships between pixels. The set of C_k found is non-disjoint. See Fig 5's caption to understand it better.\n",
    "\n",
    "\n",
    "The segmentation method used are not consistent. Sometimes they use gPb, a tree method, sometimes they produce multiple levels, using felzenszwalb and huttenlocher. But anyway.\n",
    "\n",
    "\n",
    "\n",
    "pp. 1\n",
    "\n",
    "> A striking characteristic of the system proposed here is that the use of a large contextual window to label pixels reduces the requirement for sophisticated postprocessing methods that ensure the consistency of the labeling.\n",
    "\n",
    "However, I think postprocessing is sophisticated here as well\n",
    "\n",
    "Section 4.1 Superpixel method\n",
    "\n",
    "They trained a classifier in superpixel method. Maybe this is better than using classifer from the per-pixel classifer.\n",
    "\n",
    "Seciton 4.2 CRF method\n",
    "\n",
    "Eq. (15) since i and j are neighbors, using graideint of i or j in energy function shouldn't matter.\n",
    "\n",
    "Section 4.3.2 Cover method.\n",
    "\n",
    "To compute S, You always first restrict feature map vectors at locations covered by $C_k$, and then do spatial pyramid pooling with 3x3 (See Fig 6 caption) bins. Then we can predict its class to get purity.},\n",
    "keywords = {deep learning},\n",
    "doi = {10.1109/TPAMI.2012.231},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-17T21:27:20GMT},\n",
    "date-modified = {2017-02-21T15:07:35GMT},\n",
    "url = {http://ieeexplore.ieee.org/document/6338939/},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Farabet/TPAMI%202013%20Farabet.pdf},\n",
    "file = {{TPAMI 2013 Farabet.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2013/Farabet/TPAMI 2013 Farabet.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1109/TPAMI.2012.231}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "### Multiresolution CNN, PhD thesis\n",
    "\n",
    "C. Farabet, “Towards Real-Time Image Understanding with Convolutional Networks. (Analyse sémantique des images en temps-réel avec des réseaux convolutifs),” University of Paris-Est, France, 2013.\n",
    "\n",
    "Notes: PhD thesis corresponding to Learning Hierarchical Features for Scene Labeling. Also there's some discussion about hardware.\n",
    "\n",
    "~~~\n",
    "@phdthesis{Farabet:2013ux,\n",
    "author = {Farabet, Cl{\\'e}ment},\n",
    "title = {{Towards Real-Time Image Understanding with Convolutional Networks. (Analyse s{\\'e}mantique des images en temps-r{\\'e}el avec des r{\\'e}seaux convolutifs)}},\n",
    "school = {University of Paris-Est, France},\n",
    "year = {2013},\n",
    "annote = {PhD thesis corresponding to Learning Hierarchical Features for Scene Labeling. Also there's some discussion about hardware.},\n",
    "publisher = {University of Paris-Est, France},\n",
    "keywords = {deep learning},\n",
    "language = {English},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-20T22:08:21GMT},\n",
    "date-modified = {2017-02-21T15:08:58GMT},\n",
    "abstract = {One of the open questions of artificial computer vision is how to produce good internal representations of the visual world. What sort of internal representation would allow an artificial vision system to detect and classify objects into categories, independently of pose, scale, illumination, conformation, and clutter ? More interestingly, how could an artificial vision system {em learn} appropriate internal representations automatically, the way animals and humans seem to learn by simply looking at the world ? Another related question is that of computational tractability, and more precisely that of computational efficiency. Given a good visual representation, how efficiently can it be trained, and used to encode new sensorial data. Efficiency has several dimensions: power requirements, processing speed, and memory usage. In this thesis I present three new contributions to the field of computer vision:(1) a multiscale deep convolutional network architecture to easily capture long-distance relationships between input variables in image data, (2) a tree-based algorithm to efficiently explore multiple segmentation candidates, to produce maximally confident semantic segmentations of images,(3) a custom dataflow computer architecture optimized for the computation of convolutional networks, and similarly dense image processing models. All three contributions were produced with the common goal of getting us closer to real-time image understanding. Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. In the first part of this thesis, I propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. In parallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average \"purity\" of the class distributions, hence maximizing the overall likelihood that each segment contains a single object. The system yields record accuracies on several public benchmarks. The computation of convolutional networks, and related models heavily relies on a set of basic operators that are particularly fit for dedicated hardware implementations. In the second part of this thesis I introduce a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms, neuFlow, and a dataflow compiler, luaFlow, that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications (results from 2011)},\n",
    "url = {https://tel.archives-ouvertes.fr/tel-00965622},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Books/2013/Farabet/2013%20Farabet.pdf},\n",
    "file = {{2013 Farabet.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Books/2013/Farabet/2013 Farabet.pdf:application/pdf;2013 Farabet.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Books/2013/Farabet/2013 Farabet.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/34FE8E01-C57B-45F2-8CE5-B60644DCF45B}}\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning methods\n",
    "\n",
    "One paper missing is the Hypercolumn paper (\"Hypercolumns for object segmentation and fine-grained localization\"), which I put under \"Features\".\n",
    "\n",
    "## Fully convolutional networks (FCN)\n",
    "\n",
    "J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3431–3440.\n",
    "\n",
    "Notes: Some good ideas are proposed in this paper, such as deconvolution, skip connection, end-to-end training, etc.\n",
    "\n",
    "Bilinear sampling can be implemented in \"deconvolution layer\". Check <https://github.com/Theano/Theano/blob/rel-0.8.2/theano/tensor/nnet/abstract_conv.py> for some ideas. However, I think the deconv layer's parameters must allow this. For example, if stride is bigger than kernel size, then there's no possibility of doing bilinear interpolation. No idea how Caffe handles this.\n",
    "\n",
    "pp. 3  Second equation is essentially what I derived in <https://github.com/leelabcnbc/leelab-toolbox/blob/master/leelabtoolbox/feature_extraction/cnn/cnnsizehelper.py>\n",
    "\n",
    "On detail how to combine different layers, check the protofile themselves at <fcn.berkeleyvision.org>\n",
    "\n",
    "<https://zhuanlan.zhihu.com/p/22976342> is a good description of some implementation details of FCN. A copy is at [here](./_fcn/fcn_details.htm)\n",
    "\n",
    "Section 3.2\n",
    "\n",
    "Shift-and-stitch is probably the best name for this trick, which is also mentioned in OverFeat paper.\n",
    "\n",
    "pp. 4 The first equation is a mathematically equivalent expression of this trick.\n",
    "\n",
    "Section 3.3\n",
    "\n",
    "See cs231n, or matconvnet for this operation's details.\n",
    "\n",
    "pp. 6 footnote 6\n",
    "\n",
    "I guess it's \"difficult\" because poor performance. I think probably because softmax's gradient involve all K inputs, and combining that with max may not be very stable.\n",
    "\n",
    "Check the protofile, the fusion is done by summing the raw scores (without softmax), not the normalized probablities. I think this is also for numerical stability.\n",
    "\n",
    "\n",
    "pp. 7 They find that patch sampling bias doesn't affect training much.\n",
    "\n",
    "\n",
    "other notes\n",
    "\n",
    "* p.3432: We define a skip architecture to take advantage of this feature spectrum that combines deep, coarse, semantic information and shallow, fine, appearance information -- Highlighted Feb 21, 2017\n",
    "* p.3432: Common elements of these approaches include• small models restricting capacity and receptive fields;• patchwise training [27, 2, 7, 28, 9];• post-processing by superpixel projection, random fieldregularization, filtering, or local classification [7, 2, 9];• input shifting and output interlacing for dense output [29,28, 9];• multi-scale pyramid processing [7, 28, 9];• saturating tanh nonlinearities [7, 4, 28]; and• ensembles [2, 9],whereas our method does without this machinery. -- Highlighted Feb 21, 2017\n",
    "* p.3432: We fuse features across layers to define a nonlinear localto-global representation that we tune end-to-end. -- Highlighted Feb 21, 2017\n",
    "* p.3433: Dense predictions can be obtained from coarse outputs by stitching together output from shifted versions of the input. -- Highlighted Feb 21, 2017\n",
    "* p.3434: Although performing this transformation na ̈ıvely increases the cost by a factor of f 2 , there is a well-known trick for efficiently producing identical results [11, 29] known to the wavelet community as the a` trous algorithm [25]. -- Highlighted Feb 21, 2017\n",
    "* p.3437: Final layer deconvolutional filters are fixed to bilinear interpolation, while intermediate upsampling layers are initialized to bilinear upsampling, and then learned. -- Highlighted Feb 21, 2017\n",
    "\n",
    "\n",
    "~~~\n",
    "@inproceedings{Long:2015cs,\n",
    "author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},\n",
    "title = {{Fully convolutional networks for semantic segmentation}},\n",
    "booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "year = {2015},\n",
    "pages = {3431--3440},\n",
    "publisher = {IEEE},\n",
    "annote = {Some good ideas are proposed in this paper, such as deconvolution, skip connection, end-to-end training, etc.\n",
    "\n",
    "Bilinear sampling can be implemented in \"deconvolution layer\". Check <https://github.com/Theano/Theano/blob/rel-0.8.2/theano/tensor/nnet/abstract_conv.py> for some ideas. However, I think the deconv layer's parameters must allow this. For example, if stride is bigger than kernel size, then there's no possibility of doing bilinear interpolation. No idea how Caffe handles this.\n",
    "\n",
    "\n",
    "pp. 3  Second equation is essentially what I derived in <https://github.com/leelabcnbc/leelab-toolbox/blob/master/leelabtoolbox/feature_extraction/cnn/cnnsizehelper.py>\n",
    "\n",
    "\n",
    "\n",
    "On detail how to combine different layers, check the protofile themselves at <fcn.berkeleyvision.org>\n",
    "\n",
    "<https://zhuanlan.zhihu.com/p/22976342> is a good description of some implementation details of FCN.\n",
    "\n",
    "\n",
    "Section 3.2\n",
    "\n",
    "Shift-and-stitch is probably the best name for this trick, which is also mentioned in OverFeat paper.\n",
    "\n",
    "pp. 4 The first equation is a mathematically equivalent expression of this trick.\n",
    "\n",
    "Section 3.3\n",
    "\n",
    "See cs231n, or matconvnet for this operation's details.\n",
    "\n",
    "pp. 6 footnote 6\n",
    "\n",
    "I guess it's \"difficult\" because poor performance. I think probably because softmax's gradient involve all K inputs, and combining that with max may not be very stable.\n",
    "\n",
    "Check the protofile, the fusion is done by summing the raw scores (without softmax), not the normalized probablities. I think this is also for numerical stability.\n",
    "\n",
    "\n",
    "pp. 7 They find that patch sampling bias doesn't affect training much.\n",
    "},\n",
    "keywords = {deep learning, hierarchical},\n",
    "doi = {10.1109/CVPR.2015.7298965},\n",
    "isbn = {978-1-4673-6964-0},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-02-20T19:06:56GMT},\n",
    "date-modified = {2017-02-21T23:10:01GMT},\n",
    "url = {http://ieeexplore.ieee.org/document/7298965/},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Long/CVPR%202015%202015%20Long.pdf},\n",
    "file = {{CVPR 2015 2015 Long.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Long/CVPR 2015 2015 Long.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1109/CVPR.2015.7298965}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## Pyramid Scene Parsing Network\n",
    "\n",
    "H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid Scene Parsing Network,” arXiv, vol. cs.CV, Dec. 2016.\n",
    "\n",
    "Notes: devil is in the details\n",
    "\n",
    "the idea isn't new at all. However, some implementation works, some doesn't.\n",
    "\n",
    "the motivation in Section 3.1 to me is applicable to all object detection works. Not specific to PSPnet\n",
    "\n",
    "~~~\n",
    "@article{Zhao:2016vu,\n",
    "author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},\n",
    "title = {{Pyramid Scene Parsing Network}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2016},\n",
    "volume = {cs.CV},\n",
    "month = dec,\n",
    "annote = {devil is in the details\n",
    "\n",
    "the idea isn't new at all. However, some implementation works, some doesn't.\n",
    "\n",
    "the motivation in Section 3.1 to me is applicable to all object detection works. Not specific to PSPnet},\n",
    "keywords = {deep learning},\n",
    "read = {Yes},\n",
    "rating = {3},\n",
    "date-added = {2017-02-23T17:49:41GMT},\n",
    "date-modified = {2017-04-25T16:29:54GMT},\n",
    "url = {http://arxiv.org/abs/1612.01105},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Zhao/arXiv%202016%20Zhao.pdf},\n",
    "file = {{arXiv 2016 Zhao.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2016/Zhao/arXiv 2016 Zhao.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/0059DEBC-ADF7-4A92-B233-98D3C6D7336C}}\n",
    "}\n",
    "~~~\n",
    "\n",
    "## dilated convolution\n",
    "\n",
    "F. Yu and V. Koltun, “Multi-Scale Context Aggregation by Dilated Convolutions,” arXiv, vol. cs.CV, Nov. 2015.\n",
    "\n",
    "Notes: simple pipeline beats complex ones, beating all previous results.\n",
    "\n",
    "Math of dilated conv.\n",
    "\n",
    "You can find it at <http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/>.\n",
    "\n",
    "Or you can simply check Eqs. (1) and (2)\n",
    "\n",
    "Here I believe they assume standard stride=1 convolution.\n",
    "\n",
    "In (1), s=p-t,\n",
    "In (2) s=p-lt. That's why we have (l-1) zeros inserted between filter weights if we convert a dilated conv into a normal conv.\n",
    "\n",
    "You can also check <https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d>\n",
    "\n",
    "> This is equivalent to convolving the input with a set of upsampled filters, produced by inserting rate - 1 zeros between two consecutive values of the filters along the height and width dimensions, hence the name atrous convolution or convolution with holes (the French word trous means holes in English).\n",
    "\n",
    "I think in Caffe, when you set `dilation`, then the padding should be set as if you are convolving with dilated filter (inserted with zero). So if you set kernel size to be 3, and dilation to be 2, then you should set padding as if kernel size is 5.\n",
    "\n",
    "\n",
    "In Section 3, their initialization is essentially identity matrix (Eq. 4) or generalized identity matrix (Eq. 5). In Eq. 5, for other entries, noise is introduced. I believe this is also the case for Eq. 4.\n",
    "\n",
    "~~~\n",
    "@article{Yu:2015uc,\n",
    "author = {Yu, Fisher and Koltun, Vladlen},\n",
    "title = {{Multi-Scale Context Aggregation by Dilated Convolutions}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2015},\n",
    "volume = {cs.CV},\n",
    "month = nov,\n",
    "annote = {simple pipeline beats complex ones, beating all previous results.\n",
    "\n",
    "Math of dilated conv.\n",
    "\n",
    "You can find it at <http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/>.\n",
    "\n",
    "Or you can simply check Eqs. (1) and (2)\n",
    "\n",
    "Here I believe they assume standard stride=1 convolution.\n",
    "\n",
    "In (1), s=p-t,\n",
    "In (2) s=p-lt. That's why we have (l-1) zeros inserted between filter weights if we convert a dilated conv into a normal conv.\n",
    "\n",
    "You can also check <https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d>\n",
    "\n",
    "> This is equivalent to convolving the input with a set of upsampled filters, produced by inserting rate - 1 zeros between two consecutive values of the filters along the height and width dimensions, hence the name atrous convolution or convolution with holes (the French word trous means holes in English).\n",
    "\n",
    "In Section 3, their initialization is essentially identity matrix (Eq. 4) or generalized identity matrix (Eq. 5). In Eq. 5, for other entries, noise is introduced. I believe this is also the case for Eq. 4.\n",
    "\n",
    "\n",
    "},\n",
    "keywords = {context, deep learning},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-04-25T16:41:26GMT},\n",
    "date-modified = {2017-04-25T17:15:53GMT},\n",
    "url = {http://arxiv.org/abs/1511.07122},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Yu/arXiv%202015%20Yu.pdf},\n",
    "file = {{arXiv 2015 Yu.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Yu/arXiv 2015 Yu.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/0CAFD67C-1F39-4395-AFCB-59D16953E26E}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
